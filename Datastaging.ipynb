{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01a2652f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as fn\n",
    "import pyspark.sql.types\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d3e4bd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Filters years from 2005-2020\n",
    "# 2. Unbucketize the columns\n",
    "# 3. Filters chosen countries\n",
    "def filterCountryData(df,countries_chosen):\n",
    "    \n",
    "    # range b/w 2005-2020\n",
    "    years = list(map(lambda x: str(x),list(range(2005,2021,1)))) \n",
    "    \n",
    "    cols =[\"Country Name\",\"Country Code\",\"Indicator Name\",\"Indicator Code\"]+years\n",
    "    country_2005_20 = df.select(cols)\n",
    "    \n",
    "    \n",
    "    # filters countries chosen and fills any missing year values with 0.00\n",
    "    ts = \"2020-04-01\"\n",
    "    countries_chosen_2005_20 = (country_2005_20\n",
    "                                .filter(fn.col(\"Country Name\").isin(countries_chosen)).fillna(0.00, subset=years)\n",
    "                                .withColumn(\"date\",fn.date_format(fn.lit(ts),\"yyyy-MM-dd\"))\n",
    "                               )\n",
    "    \n",
    "    #unbucketize the data\n",
    "    unpivotStr= list(map(lambda x: \" '{t}',`{t}`\".format(t=x),years))\n",
    "    sep = ','\n",
    "    unpivotExpr = \"stack(\"+str(len(years))+\", \"+sep.join(unpivotStr)+\") as (Year, Value)\"\n",
    "    columns_without_years= set(countries_chosen_2005_20.columns ) - set(years)\n",
    "    \n",
    "    res = countries_chosen_2005_20.select(\n",
    "        \"Country Name\",\n",
    "        \"Country Code\",\n",
    "        \"Indicator Name\",\n",
    "        fn.expr(unpivotExpr),\n",
    "        fn.month(\"date\").alias(\"month\"),\n",
    "        fn.dayofmonth(\"date\").alias(\"day\"),\n",
    "        fn.quarter(\"date\").alias(\"quarter\")            \n",
    "    ).groupBy(\"Country Name\",\"Year\").pivot(\"Indicator Name\").sum(\"Value\")\n",
    "    \n",
    "    #TODO: join the dimensions to make a fact table\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "002bfeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date Dimension\n",
    "def generate_dates(spark,range_list,interval=60*60*24,dt_col=\"date_time_ref\"): # TODO: attention to sparkSession\n",
    "     \"\"\"\n",
    "     Create a Spark DataFrame with a single column named dt_col and a range of date within a specified interval (start and stop included).\n",
    "     With hourly data, dates end at 23 of stop day\n",
    "\n",
    "     :param spark: SparkSession or sqlContext depending on environment (server vs local)\n",
    "     :param range_list: array of strings formatted as \"2018-01-20\" or \"2018-01-20 00:00:00\"\n",
    "     :param interval: number of seconds (frequency), output from get_freq()\n",
    "     :param dt_col: string with date column name. Date column must be TimestampType\n",
    "\n",
    "     :returns: df from range\n",
    "     \"\"\"\n",
    "     start,stop = range_list\n",
    "     temp_df = spark.createDataFrame([(start, stop)], (\"start\", \"stop\"))\n",
    "     temp_df = temp_df.select([fn.col(c).cast(\"timestamp\") for c in (\"start\", \"stop\")])\n",
    "     temp_df = temp_df.withColumn(\"stop\",fn.date_add(\"stop\",1).cast(\"timestamp\"))\n",
    "     temp_df = temp_df.select([fn.col(c).cast(\"long\") for c in (\"start\", \"stop\")])\n",
    "     start, stop = temp_df.first()\n",
    "     return spark.range(start,stop,interval).select(fn.col(\"id\").cast(\"timestamp\").alias(dt_col))\n",
    "\n",
    "\n",
    "def dateDimension():\n",
    "    time_rng = [\"2005-01-01\",\"2020-12-31\"]\n",
    "    year_df= generate_dates(spark,time_rng)\n",
    "    tmp = (year_df\n",
    "           .withColumn(\"year\",fn.year(\"date_time_ref\"))\n",
    "           .withColumn(\"month\",fn.month(\"date_time_ref\"))\n",
    "           .withColumn(\"day\",fn.dayofmonth(\"date_time_ref\"))\n",
    "           .withColumn(\"quarter\",fn.quarter(\"date_time_ref\"))\n",
    "           .withColumn(\"decade\",\n",
    "                          fn.when(fn.col(\"year\") % 10 >=5,fn.col(\"year\")-fn.col(\"year\")%10+10)\n",
    "                              .otherwise(fn.col(\"year\")- fn.col(\"year\") % 10))\n",
    "           .withColumn(\"year_code\",fn.monotonically_increasing_id())\n",
    "\n",
    "          )\n",
    "    date_dim = (tmp\n",
    "                   .select(tmp.year_code,*set(tmp.columns)-set([\"year_code\"]))\n",
    "               )\n",
    "    \n",
    "    return date_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb726a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naturalDisasterDim(df,filePath,countries_chosen):\n",
    "    \"\"\"\n",
    "        creates natural disaster dimension + look up table\n",
    "    \n",
    "        df - date dataframe\n",
    "        filePath - filePath to natural disaster csv\n",
    "        countries_chosen - list of strings of countries to work on\n",
    "    \"\"\"\n",
    "    \n",
    "    columns = [\"total deaths\",\"Total Damages ('000 US$)\"]\n",
    "    \n",
    "    # reads csv\n",
    "    natural_disaster_df = (spark\n",
    "                       .read\n",
    "                       .format('csv')\n",
    "                           .option(\"inferSchema\",True)\n",
    "                           .option(\"header\",True)\n",
    "                           .load(filePath)\n",
    "                           .fillna(0.00,subset=columns)).dropDuplicates()\n",
    "    \n",
    "\n",
    "    # reconfigures column names + banding\n",
    "    tmp_nd = (natural_disaster_df\n",
    "                  # replaces United States of America -> united states\n",
    "              .withColumn(\"Country\",fn.when(fn.lower(fn.col(\"Country\")).contains(\"united states\"),\"united states\").otherwise(fn.lower(fn.col(\"Country\"))))\n",
    "              .withColumn(\"start_month\",fn.col(\"Start Month\"))\n",
    "                  .withColumn(\"start_year\",fn.col(\"Start Year\"))\n",
    "                  .withColumn(\"start_day\",fn.col(\"Start Day\"))\n",
    "                  .withColumn(\"end_month\",fn.col(\"End Month\"))\n",
    "                  .withColumn(\"end_year\",fn.col(\"End Year\"))\n",
    "                  .withColumn(\"end_day\",fn.col(\"End Day\"))\n",
    "              .withColumn(\"disaster_type\",fn.col(\"Disaster Type\"))\n",
    "              .withColumn(\"disaster_subtype\",fn.col(\"disaster subtype\"))\n",
    "              .withColumn(\"disaster_nestedsubtype\",fn.col(\"disaster subsubtype\"))\n",
    "              .withColumn(\"disaster_subgroup\",fn.col(\"disaster subgroup\"))\n",
    "              .withColumn(\"event_name\",fn.col(\"event name\"))\n",
    "              .withColumn(\"ofda_response\",fn.col(\"ofda response\"))\n",
    "              .fillna(1.0,[\"start_day\",\"start_month\",\"start_year\",\"end_day\",\"end_month\",\"end_year\"])\n",
    "              # TODO figure out what to do about start and end year\n",
    "              .fillna(\"Not Available\",[\"disaster_type\",\"disaster_subtype\",\"disaster_nestedsubtype\",\"disaster_subgroup\",\"event_name\",\"ofda_response\"])\n",
    "              .withColumn(\"ttl_death\",\n",
    "                          # range (low,medium, high)\n",
    "                          fn.when(fn.col(\"total deaths\")>7000,\n",
    "                                  fn.when(fn.col(\"total deaths\")>14000,\"high\").otherwise(\"medium\")).otherwise(\"low\")\n",
    "                         )\n",
    "              .withColumn(\"ttl_damages\",\n",
    "                          # \n",
    "                          fn.when(fn.col(\"Total Damages ('000 US$)\")>1000000,\n",
    "                                  fn.when(fn.col(\"Total Damages ('000 US$)\")>100000000,\"high\").otherwise(\"medium\")).otherwise(\"low\")\n",
    "                         )\n",
    "              \n",
    "                  .drop(\"year\")\n",
    "             )\n",
    "\n",
    "    # join on start year\n",
    "    max_year = df.select(fn.max(\"year\")).limit(1).collect()[0][0]\n",
    "    min_year = df.select(fn.min(\"year\")).limit(1).collect()[0][0]\n",
    "    \n",
    "    nd_j_on_date = tmp_nd.filter(fn.col(\"start_year\")>=min_year).filter(fn.col(\"end_year\")<=max_year)\n",
    "\n",
    "    # filter countries chosen\n",
    "    filtered_byCountry_date = (nd_j_on_date\n",
    "           .filter(fn.col(\"Country\").isin(list(map(lambda x: x.lower(),countries_chosen))))\n",
    "           \n",
    "    )\n",
    "    \n",
    "    # distinct banded rows with key\n",
    "    res = (filtered_byCountry_date  \n",
    "                                  .select([\n",
    "                                           \"disaster_type\",\n",
    "                                           \"disaster_subtype\",\n",
    "                                           \"disaster_nestedsubtype\",\n",
    "                                           \"disaster_subgroup\",\n",
    "                                           \"event_name\",\n",
    "                                           \"ttl_death\",\n",
    "                                           \"ttl_damages\",\n",
    "                                           \"ofda_response\"])                                   \n",
    "                                 ).distinct().withColumn(\"natural_disaster_key\",fn.monotonically_increasing_id())\n",
    "    \n",
    "    \n",
    "    lookup=(res.join(\n",
    "        filtered_byCountry_date,\n",
    "        on = [\n",
    "            \"disaster_type\",\"disaster_subtype\",\"disaster_nestedsubtype\",\"disaster_subgroup\",\"event_name\",\"ofda_response\",\"ttl_damages\",\"ttl_death\"\n",
    "        ])\n",
    "        .select(\"natural_disaster_key\",\"Country\",\"start_year\",\"start_month\",\"start_day\",\"end_year\",\"end_month\",\"end_day\")\n",
    "        )\n",
    "    \n",
    "    \n",
    "    # dimension, lookup\n",
    "    return res,lookup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "41172b1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country_name</th>\n",
       "      <th>currency</th>\n",
       "      <th>region</th>\n",
       "      <th>population_total</th>\n",
       "      <th>population_growth</th>\n",
       "      <th>urban_population_growth</th>\n",
       "      <th>urban_population</th>\n",
       "      <th>rural_population</th>\n",
       "      <th>unemployment_rate</th>\n",
       "      <th>age_dependency_ratio_workingage</th>\n",
       "      <th>poverty_headcount_percentage</th>\n",
       "      <th>labor_force_total</th>\n",
       "      <th>net_migration</th>\n",
       "      <th>country_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nigeria</td>\n",
       "      <td>nigerian naira</td>\n",
       "      <td>Sub-Saharan Africa</td>\n",
       "      <td>176404931.0</td>\n",
       "      <td>2.665007</td>\n",
       "      <td>4.521129</td>\n",
       "      <td>82878565.0</td>\n",
       "      <td>93526366.0</td>\n",
       "      <td>4.56</td>\n",
       "      <td>88.498487</td>\n",
       "      <td>0.0</td>\n",
       "      <td>medium</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>somalia</td>\n",
       "      <td>somali shilling</td>\n",
       "      <td>Sub-Saharan Africa</td>\n",
       "      <td>13423571.0</td>\n",
       "      <td>2.717396</td>\n",
       "      <td>4.042955</td>\n",
       "      <td>5729046.0</td>\n",
       "      <td>7694525.0</td>\n",
       "      <td>13.32</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>low</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>finland</td>\n",
       "      <td>euro</td>\n",
       "      <td>Europe &amp; Central Asia</td>\n",
       "      <td>5313399.0</td>\n",
       "      <td>0.465549</td>\n",
       "      <td>0.622925</td>\n",
       "      <td>4426008.0</td>\n",
       "      <td>887391.0</td>\n",
       "      <td>6.37</td>\n",
       "      <td>50.079271</td>\n",
       "      <td>13.8</td>\n",
       "      <td>low</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17179869184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mexico</td>\n",
       "      <td>mexican peso</td>\n",
       "      <td>Latin America &amp; Caribbean</td>\n",
       "      <td>121858251.0</td>\n",
       "      <td>1.241165</td>\n",
       "      <td>1.610136</td>\n",
       "      <td>96615314.0</td>\n",
       "      <td>25242937.0</td>\n",
       "      <td>4.31</td>\n",
       "      <td>52.347721</td>\n",
       "      <td>0.0</td>\n",
       "      <td>medium</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34359738368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>thailand</td>\n",
       "      <td>thai baht</td>\n",
       "      <td>East Asia &amp; Pacific</td>\n",
       "      <td>68971313.0</td>\n",
       "      <td>0.373015</td>\n",
       "      <td>1.941560</td>\n",
       "      <td>33415222.0</td>\n",
       "      <td>35556091.0</td>\n",
       "      <td>0.69</td>\n",
       "      <td>40.157546</td>\n",
       "      <td>8.6</td>\n",
       "      <td>medium</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51539607552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>canada</td>\n",
       "      <td>canadian dollar</td>\n",
       "      <td>North America</td>\n",
       "      <td>35702908.0</td>\n",
       "      <td>0.746339</td>\n",
       "      <td>0.796808</td>\n",
       "      <td>29011826.0</td>\n",
       "      <td>6691082.0</td>\n",
       "      <td>6.91</td>\n",
       "      <td>47.156456</td>\n",
       "      <td>0.0</td>\n",
       "      <td>low</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1666447310848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>norway</td>\n",
       "      <td>norwegian krone</td>\n",
       "      <td>Europe &amp; Central Asia</td>\n",
       "      <td>5379475.0</td>\n",
       "      <td>0.588757</td>\n",
       "      <td>1.021155</td>\n",
       "      <td>4463566.0</td>\n",
       "      <td>915909.0</td>\n",
       "      <td>4.62</td>\n",
       "      <td>53.335836</td>\n",
       "      <td>0.0</td>\n",
       "      <td>low</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1666447310849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>mexico</td>\n",
       "      <td>mexican peso</td>\n",
       "      <td>Latin America &amp; Caribbean</td>\n",
       "      <td>107560155.0</td>\n",
       "      <td>1.456213</td>\n",
       "      <td>1.859028</td>\n",
       "      <td>82408288.0</td>\n",
       "      <td>25151867.0</td>\n",
       "      <td>3.57</td>\n",
       "      <td>59.071730</td>\n",
       "      <td>0.0</td>\n",
       "      <td>medium</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1675037245440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>somalia</td>\n",
       "      <td>somali shilling</td>\n",
       "      <td>Sub-Saharan Africa</td>\n",
       "      <td>10763904.0</td>\n",
       "      <td>2.989724</td>\n",
       "      <td>5.139336</td>\n",
       "      <td>3993408.0</td>\n",
       "      <td>6770496.0</td>\n",
       "      <td>13.19</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>low</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1675037245441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>finland</td>\n",
       "      <td>euro</td>\n",
       "      <td>Europe &amp; Central Asia</td>\n",
       "      <td>5338871.0</td>\n",
       "      <td>0.478246</td>\n",
       "      <td>0.634201</td>\n",
       "      <td>4454167.0</td>\n",
       "      <td>884704.0</td>\n",
       "      <td>8.25</td>\n",
       "      <td>50.321218</td>\n",
       "      <td>13.1</td>\n",
       "      <td>low</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1700807049216</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>144 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    country_name         currency                     region  \\\n",
       "0        nigeria   nigerian naira         Sub-Saharan Africa   \n",
       "1        somalia  somali shilling         Sub-Saharan Africa   \n",
       "2        finland             euro      Europe & Central Asia   \n",
       "3         mexico     mexican peso  Latin America & Caribbean   \n",
       "4       thailand        thai baht        East Asia & Pacific   \n",
       "..           ...              ...                        ...   \n",
       "139       canada  canadian dollar              North America   \n",
       "140       norway  norwegian krone      Europe & Central Asia   \n",
       "141       mexico     mexican peso  Latin America & Caribbean   \n",
       "142      somalia  somali shilling         Sub-Saharan Africa   \n",
       "143      finland             euro      Europe & Central Asia   \n",
       "\n",
       "     population_total  population_growth  urban_population_growth  \\\n",
       "0         176404931.0           2.665007                 4.521129   \n",
       "1          13423571.0           2.717396                 4.042955   \n",
       "2           5313399.0           0.465549                 0.622925   \n",
       "3         121858251.0           1.241165                 1.610136   \n",
       "4          68971313.0           0.373015                 1.941560   \n",
       "..                ...                ...                      ...   \n",
       "139        35702908.0           0.746339                 0.796808   \n",
       "140         5379475.0           0.588757                 1.021155   \n",
       "141       107560155.0           1.456213                 1.859028   \n",
       "142        10763904.0           2.989724                 5.139336   \n",
       "143         5338871.0           0.478246                 0.634201   \n",
       "\n",
       "     urban_population  rural_population  unemployment_rate  \\\n",
       "0          82878565.0        93526366.0               4.56   \n",
       "1           5729046.0         7694525.0              13.32   \n",
       "2           4426008.0          887391.0               6.37   \n",
       "3          96615314.0        25242937.0               4.31   \n",
       "4          33415222.0        35556091.0               0.69   \n",
       "..                ...               ...                ...   \n",
       "139        29011826.0         6691082.0               6.91   \n",
       "140         4463566.0          915909.0               4.62   \n",
       "141        82408288.0        25151867.0               3.57   \n",
       "142         3993408.0         6770496.0              13.19   \n",
       "143         4454167.0          884704.0               8.25   \n",
       "\n",
       "     age_dependency_ratio_workingage  poverty_headcount_percentage  \\\n",
       "0                          88.498487                           0.0   \n",
       "1                         100.000000                           0.0   \n",
       "2                          50.079271                          13.8   \n",
       "3                          52.347721                           0.0   \n",
       "4                          40.157546                           8.6   \n",
       "..                               ...                           ...   \n",
       "139                        47.156456                           0.0   \n",
       "140                        53.335836                           0.0   \n",
       "141                        59.071730                           0.0   \n",
       "142                       100.000000                           0.0   \n",
       "143                        50.321218                          13.1   \n",
       "\n",
       "    labor_force_total  net_migration    country_key  \n",
       "0              medium            0.0              0  \n",
       "1                 low            0.0              1  \n",
       "2                 low            0.0    17179869184  \n",
       "3              medium            0.0    34359738368  \n",
       "4              medium            0.0    51539607552  \n",
       "..                ...            ...            ...  \n",
       "139               low            0.0  1666447310848  \n",
       "140               low            0.0  1666447310849  \n",
       "141            medium            0.0  1675037245440  \n",
       "142               low            0.0  1675037245441  \n",
       "143               low            0.0  1700807049216  \n",
       "\n",
       "[144 rows x 14 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country_name</th>\n",
       "      <th>year</th>\n",
       "      <th>country_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nigeria</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>somalia</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>finland</td>\n",
       "      <td>2008</td>\n",
       "      <td>17179869184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mexico</td>\n",
       "      <td>2015</td>\n",
       "      <td>34359738368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>thailand</td>\n",
       "      <td>2016</td>\n",
       "      <td>51539607552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>canada</td>\n",
       "      <td>2015</td>\n",
       "      <td>1666447310848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>norway</td>\n",
       "      <td>2020</td>\n",
       "      <td>1666447310849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>mexico</td>\n",
       "      <td>2006</td>\n",
       "      <td>1675037245440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>somalia</td>\n",
       "      <td>2006</td>\n",
       "      <td>1675037245441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>finland</td>\n",
       "      <td>2009</td>\n",
       "      <td>1700807049216</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>144 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    country_name  year    country_key\n",
       "0        nigeria  2014              0\n",
       "1        somalia  2014              1\n",
       "2        finland  2008    17179869184\n",
       "3         mexico  2015    34359738368\n",
       "4       thailand  2016    51539607552\n",
       "..           ...   ...            ...\n",
       "139       canada  2015  1666447310848\n",
       "140       norway  2020  1666447310849\n",
       "141       mexico  2006  1675037245440\n",
       "142      somalia  2006  1675037245441\n",
       "143      finland  2009  1700807049216\n",
       "\n",
       "[144 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def countryDimension(time_df,indicators,countries_chosen,filePath):\n",
    "    max_year = time_df.select(fn.max(\"year\")).limit(1).collect()[0][0]\n",
    "    min_year = time_df.select(fn.min(\"year\")).limit(1).collect()[0][0]\n",
    "    \n",
    "    countries = (spark\n",
    "                       .read\n",
    "                       .format('csv')\n",
    "                           .option(\"inferSchema\",True)\n",
    "                           .option(\"header\",True)\n",
    "                           .load(filePath)\n",
    "                        )\n",
    "\n",
    "    indicators = (indicators\n",
    "                  .withColumn(\"country_name\",fn.lower(\"Country Name\"))\n",
    "                  .drop(\"Country Name\")\n",
    "                  .withColumn(\"age_dependency_ratio_workingage\",\n",
    "                              fn.when(fn.col(\"Age dependency ratio (% of working-age population)\")>100.00, 100.00)\n",
    "                              .otherwise(fn.col(\"Age dependency ratio (% of working-age population)\")),)\n",
    "                  .withColumn(\"labor_force_total\",\n",
    "                              fn.when(fn.col(\"Labor force, total\")>30000000,\n",
    "                                     fn.when(fn.col(\"Labor force, total\")>80000000,\"high\").otherwise(\"medium\")\n",
    "                                     ).otherwise(\"low\"))\n",
    "                  .select(\n",
    "                      fn.col(\"country_name\"),\n",
    "                      fn.col(\"Population, total\").alias(\"population_total\"),\n",
    "                      fn.col(\"Population growth (annual %)\").alias(\"population_growth\"),\n",
    "                      fn.col(\"Urban population growth (annual %)\").alias(\"urban_population_growth\"),\n",
    "                      fn.col(\"Urban population\").alias(\"urban_population\"),\n",
    "                      fn.col(\"Rural population\").alias(\"rural_population\"),\n",
    "                      fn.col(\"Unemployment, total (% of total labor force)\").alias(\"unemployment_rate\"),\n",
    "                      fn.col(\"age_dependency_ratio_workingage\"),\n",
    "                      fn.col(\"Poverty headcount ratio at national poverty line (% of population)\").alias(\"poverty_headcount_percentage\"),\n",
    "                      fn.col(\"labor_force_total\"),\n",
    "                      fn.col(\"Net migration\").alias(\"net_migration\"),\n",
    "                      fn.col(\"year\")\n",
    "                  )\n",
    "#                   .fillna(\n",
    "#                       indicators.select(fn.avg(\"Age dependency ratio (% of working-age population)\")).collect()[0][0],\n",
    "#                       subset=[\"age_dependency_ratio_workingage\"]\n",
    "#                   )\n",
    "                 )\n",
    "    tmp = (countries\n",
    "               .filter(fn.lower(fn.col(\"short name\")).isin(list(map(lambda x: x.lower(),countries_chosen))))\n",
    "               .select(\n",
    "                   fn.lower(\"Currency Unit\").alias(\"currency\"),\n",
    "                   fn.lower(\"short name\").alias(\"country_name\"),\n",
    "                   fn.col(\"region\"),\n",
    "               )\n",
    "          )\n",
    "    \n",
    "    res = (tmp.join(indicators,on=[\"country_name\"]).withColumn(\"country_key\",fn.monotonically_increasing_id()))\n",
    "    \n",
    "    lookup = (res.select(\n",
    "        \"country_name\",\n",
    "        \"year\",\n",
    "        \"country_key\"\n",
    "    ))\n",
    "\n",
    "           \n",
    "    return res.drop(\"year\"), lookup\n",
    "               \n",
    "dateDim = dateDimension()\n",
    "df,tmp = countryDimension(  \n",
    "    dateDim,\n",
    "    filterdCountryDf,\n",
    "    countries_chosen=countries_chosen,\n",
    "    filePath=\"AssignmentData/HNP_StatsCountry.csv\")\n",
    "\n",
    "display(df.toPandas())\n",
    "display(tmp.toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b73a423",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"ds_datastage\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "7d9fd12d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "C:\\Users\\aweso\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country Name</th>\n",
       "      <th>Year</th>\n",
       "      <th>AIDS estimated deaths (UNAIDS estimates)</th>\n",
       "      <th>ARI treatment (% of children under 5 taken to a health provider)</th>\n",
       "      <th>Adolescent fertility rate (births per 1,000 women ages 15-19)</th>\n",
       "      <th>Adults (ages 15+) and children (0-14 years) living with HIV</th>\n",
       "      <th>Adults (ages 15+) and children (ages 0-14) newly infected with HIV</th>\n",
       "      <th>Adults (ages 15+) living with HIV</th>\n",
       "      <th>Adults (ages 15-49) newly infected with HIV</th>\n",
       "      <th>Age at first marriage, female</th>\n",
       "      <th>...</th>\n",
       "      <th>Urban population (% of total population)</th>\n",
       "      <th>Urban population growth (annual %)</th>\n",
       "      <th>Urban poverty headcount ratio at national poverty lines (% of urban population)</th>\n",
       "      <th>Use of insecticide-treated bed nets (% of under-5 population)</th>\n",
       "      <th>Vitamin A supplementation coverage rate (% of children ages 6-59 months)</th>\n",
       "      <th>Wanted fertility rate (births per woman)</th>\n",
       "      <th>Women who were first married by age 15 (% of women ages 20-24)</th>\n",
       "      <th>Women who were first married by age 18 (% of women ages 20-24)</th>\n",
       "      <th>Women's share of population ages 15+ living with HIV (%)</th>\n",
       "      <th>Young people (ages 15-24) newly infected with HIV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Madagascar</td>\n",
       "      <td>2015</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>113.7668</td>\n",
       "      <td>24000.0</td>\n",
       "      <td>4400.0</td>\n",
       "      <td>23000.0</td>\n",
       "      <td>3800.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>35.193</td>\n",
       "      <td>4.584426</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46.5</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mexico</td>\n",
       "      <td>2015</td>\n",
       "      <td>5900.0</td>\n",
       "      <td>73.1</td>\n",
       "      <td>62.6070</td>\n",
       "      <td>280000.0</td>\n",
       "      <td>19000.0</td>\n",
       "      <td>280000.0</td>\n",
       "      <td>17000.0</td>\n",
       "      <td>23.2</td>\n",
       "      <td>...</td>\n",
       "      <td>79.285</td>\n",
       "      <td>1.610136</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.8</td>\n",
       "      <td>26.1</td>\n",
       "      <td>18.8</td>\n",
       "      <td>4000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Madagascar</td>\n",
       "      <td>2019</td>\n",
       "      <td>1700.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>105.8848</td>\n",
       "      <td>38000.0</td>\n",
       "      <td>5800.0</td>\n",
       "      <td>36000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>37.861</td>\n",
       "      <td>4.441921</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46.7</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thailand</td>\n",
       "      <td>2016</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>79.5</td>\n",
       "      <td>46.2390</td>\n",
       "      <td>540000.0</td>\n",
       "      <td>9500.0</td>\n",
       "      <td>530000.0</td>\n",
       "      <td>9300.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>48.448</td>\n",
       "      <td>1.941560</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44.9</td>\n",
       "      <td>4900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thailand</td>\n",
       "      <td>2010</td>\n",
       "      <td>29000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.9138</td>\n",
       "      <td>620000.0</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>610000.0</td>\n",
       "      <td>14000.0</td>\n",
       "      <td>24.9</td>\n",
       "      <td>...</td>\n",
       "      <td>43.856</td>\n",
       "      <td>3.529244</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>Mexico</td>\n",
       "      <td>2006</td>\n",
       "      <td>6700.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>71.4640</td>\n",
       "      <td>200000.0</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>190000.0</td>\n",
       "      <td>14000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>76.616</td>\n",
       "      <td>1.859028</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.7</td>\n",
       "      <td>18.3</td>\n",
       "      <td>3200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>Niger</td>\n",
       "      <td>2012</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>53.1</td>\n",
       "      <td>200.7430</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>1300.0</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>17.2</td>\n",
       "      <td>...</td>\n",
       "      <td>16.212</td>\n",
       "      <td>3.874067</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>98.0</td>\n",
       "      <td>7.4</td>\n",
       "      <td>28.0</td>\n",
       "      <td>76.3</td>\n",
       "      <td>51.8</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>China</td>\n",
       "      <td>2012</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.3770</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>51.765</td>\n",
       "      <td>3.130657</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>India</td>\n",
       "      <td>2020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2300000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>34.926</td>\n",
       "      <td>2.297828</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>Guinea</td>\n",
       "      <td>2005</td>\n",
       "      <td>5700.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>159.3410</td>\n",
       "      <td>81000.0</td>\n",
       "      <td>8800.0</td>\n",
       "      <td>70000.0</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>19.3</td>\n",
       "      <td>...</td>\n",
       "      <td>32.257</td>\n",
       "      <td>2.910751</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>99.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>19.8</td>\n",
       "      <td>63.1</td>\n",
       "      <td>64.0</td>\n",
       "      <td>2900.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>144 rows × 459 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Country Name  Year  AIDS estimated deaths (UNAIDS estimates)  \\\n",
       "0     Madagascar  2015                                    1100.0   \n",
       "1         Mexico  2015                                    5900.0   \n",
       "2     Madagascar  2019                                    1700.0   \n",
       "3       Thailand  2016                                   20000.0   \n",
       "4       Thailand  2010                                   29000.0   \n",
       "..           ...   ...                                       ...   \n",
       "139       Mexico  2006                                    6700.0   \n",
       "140        Niger  2012                                    1900.0   \n",
       "141        China  2012                                       0.0   \n",
       "142        India  2020                                       0.0   \n",
       "143       Guinea  2005                                    5700.0   \n",
       "\n",
       "     ARI treatment (% of children under 5 taken to a health provider)  \\\n",
       "0                                                  0.0                  \n",
       "1                                                 73.1                  \n",
       "2                                                  0.0                  \n",
       "3                                                 79.5                  \n",
       "4                                                  0.0                  \n",
       "..                                                 ...                  \n",
       "139                                                0.0                  \n",
       "140                                               53.1                  \n",
       "141                                                0.0                  \n",
       "142                                                0.0                  \n",
       "143                                               42.0                  \n",
       "\n",
       "     Adolescent fertility rate (births per 1,000 women ages 15-19)  \\\n",
       "0                                             113.7668               \n",
       "1                                              62.6070               \n",
       "2                                             105.8848               \n",
       "3                                              46.2390               \n",
       "4                                              47.9138               \n",
       "..                                                 ...               \n",
       "139                                            71.4640               \n",
       "140                                           200.7430               \n",
       "141                                             7.3770               \n",
       "142                                             0.0000               \n",
       "143                                           159.3410               \n",
       "\n",
       "     Adults (ages 15+) and children (0-14 years) living with HIV  \\\n",
       "0                                              24000.0             \n",
       "1                                             280000.0             \n",
       "2                                              38000.0             \n",
       "3                                             540000.0             \n",
       "4                                             620000.0             \n",
       "..                                                 ...             \n",
       "139                                           200000.0             \n",
       "140                                            35000.0             \n",
       "141                                                0.0             \n",
       "142                                          2300000.0             \n",
       "143                                            81000.0             \n",
       "\n",
       "     Adults (ages 15+) and children (ages 0-14) newly infected with HIV  \\\n",
       "0                                               4400.0                    \n",
       "1                                              19000.0                    \n",
       "2                                               5800.0                    \n",
       "3                                               9500.0                    \n",
       "4                                              15000.0                    \n",
       "..                                                 ...                    \n",
       "139                                            15000.0                    \n",
       "140                                             1300.0                    \n",
       "141                                                0.0                    \n",
       "142                                                0.0                    \n",
       "143                                             8800.0                    \n",
       "\n",
       "     Adults (ages 15+) living with HIV  \\\n",
       "0                              23000.0   \n",
       "1                             280000.0   \n",
       "2                              36000.0   \n",
       "3                             530000.0   \n",
       "4                             610000.0   \n",
       "..                                 ...   \n",
       "139                           190000.0   \n",
       "140                            30000.0   \n",
       "141                                0.0   \n",
       "142                                0.0   \n",
       "143                            70000.0   \n",
       "\n",
       "     Adults (ages 15-49) newly infected with HIV  \\\n",
       "0                                         3800.0   \n",
       "1                                        17000.0   \n",
       "2                                         5000.0   \n",
       "3                                         9300.0   \n",
       "4                                        14000.0   \n",
       "..                                           ...   \n",
       "139                                      14000.0   \n",
       "140                                       1000.0   \n",
       "141                                          0.0   \n",
       "142                                          0.0   \n",
       "143                                       6000.0   \n",
       "\n",
       "     Age at first marriage, female  ...  \\\n",
       "0                              0.0  ...   \n",
       "1                             23.2  ...   \n",
       "2                              0.0  ...   \n",
       "3                              0.0  ...   \n",
       "4                             24.9  ...   \n",
       "..                             ...  ...   \n",
       "139                            0.0  ...   \n",
       "140                           17.2  ...   \n",
       "141                            0.0  ...   \n",
       "142                            0.0  ...   \n",
       "143                           19.3  ...   \n",
       "\n",
       "     Urban population (% of total population)  \\\n",
       "0                                      35.193   \n",
       "1                                      79.285   \n",
       "2                                      37.861   \n",
       "3                                      48.448   \n",
       "4                                      43.856   \n",
       "..                                        ...   \n",
       "139                                    76.616   \n",
       "140                                    16.212   \n",
       "141                                    51.765   \n",
       "142                                    34.926   \n",
       "143                                    32.257   \n",
       "\n",
       "     Urban population growth (annual %)  \\\n",
       "0                              4.584426   \n",
       "1                              1.610136   \n",
       "2                              4.441921   \n",
       "3                              1.941560   \n",
       "4                              3.529244   \n",
       "..                                  ...   \n",
       "139                            1.859028   \n",
       "140                            3.874067   \n",
       "141                            3.130657   \n",
       "142                            2.297828   \n",
       "143                            2.910751   \n",
       "\n",
       "     Urban poverty headcount ratio at national poverty lines (% of urban population)  \\\n",
       "0                                                  0.0                                 \n",
       "1                                                  0.0                                 \n",
       "2                                                  0.0                                 \n",
       "3                                                  0.0                                 \n",
       "4                                                  0.0                                 \n",
       "..                                                 ...                                 \n",
       "139                                                0.0                                 \n",
       "140                                                0.0                                 \n",
       "141                                                0.0                                 \n",
       "142                                                0.0                                 \n",
       "143                                                0.0                                 \n",
       "\n",
       "     Use of insecticide-treated bed nets (% of under-5 population)  \\\n",
       "0                                                  0.0               \n",
       "1                                                  0.0               \n",
       "2                                                  0.0               \n",
       "3                                                  0.0               \n",
       "4                                                  0.0               \n",
       "..                                                 ...               \n",
       "139                                                0.0               \n",
       "140                                               20.1               \n",
       "141                                                0.0               \n",
       "142                                                0.0               \n",
       "143                                                1.4               \n",
       "\n",
       "     Vitamin A supplementation coverage rate (% of children ages 6-59 months)  \\\n",
       "0                                                 97.0                          \n",
       "1                                                  0.0                          \n",
       "2                                                  0.0                          \n",
       "3                                                  0.0                          \n",
       "4                                                  0.0                          \n",
       "..                                                 ...                          \n",
       "139                                                0.0                          \n",
       "140                                               98.0                          \n",
       "141                                                0.0                          \n",
       "142                                                0.0                          \n",
       "143                                               99.0                          \n",
       "\n",
       "     Wanted fertility rate (births per woman)  \\\n",
       "0                                         0.0   \n",
       "1                                         0.0   \n",
       "2                                         0.0   \n",
       "3                                         0.0   \n",
       "4                                         0.0   \n",
       "..                                        ...   \n",
       "139                                       0.0   \n",
       "140                                       7.4   \n",
       "141                                       0.0   \n",
       "142                                       0.0   \n",
       "143                                       5.1   \n",
       "\n",
       "     Women who were first married by age 15 (% of women ages 20-24)  \\\n",
       "0                                                  0.0                \n",
       "1                                                  3.8                \n",
       "2                                                  0.0                \n",
       "3                                                  0.0                \n",
       "4                                                  0.0                \n",
       "..                                                 ...                \n",
       "139                                                0.0                \n",
       "140                                               28.0                \n",
       "141                                                0.0                \n",
       "142                                                0.0                \n",
       "143                                               19.8                \n",
       "\n",
       "     Women who were first married by age 18 (% of women ages 20-24)  \\\n",
       "0                                                  0.0                \n",
       "1                                                 26.1                \n",
       "2                                                  0.0                \n",
       "3                                                  0.0                \n",
       "4                                                  0.0                \n",
       "..                                                 ...                \n",
       "139                                               22.7                \n",
       "140                                               76.3                \n",
       "141                                                0.0                \n",
       "142                                                0.0                \n",
       "143                                               63.1                \n",
       "\n",
       "     Women's share of population ages 15+ living with HIV (%)  \\\n",
       "0                                                 46.5          \n",
       "1                                                 18.8          \n",
       "2                                                 46.7          \n",
       "3                                                 44.9          \n",
       "4                                                 45.8          \n",
       "..                                                 ...          \n",
       "139                                               18.3          \n",
       "140                                               51.8          \n",
       "141                                                0.0          \n",
       "142                                                0.0          \n",
       "143                                               64.0          \n",
       "\n",
       "     Young people (ages 15-24) newly infected with HIV  \n",
       "0                                               1000.0  \n",
       "1                                               4000.0  \n",
       "2                                               1000.0  \n",
       "3                                               4900.0  \n",
       "4                                               6900.0  \n",
       "..                                                 ...  \n",
       "139                                             3200.0  \n",
       "140                                              200.0  \n",
       "141                                                0.0  \n",
       "142                                                0.0  \n",
       "143                                             2900.0  \n",
       "\n",
       "[144 rows x 459 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#MAIN block\n",
    "countries_chosen = [\"United States\", \"Canada\",\"Mexico\",\"Thailand\",\"China\",\"India\",\"Niger\",\"Madagascar\",\"Guinea\"]\n",
    "\n",
    "df=spark.read.format(\"csv\").option(\"header\",True).option(\"inferSchema\",True).load(\"AssignmentData/HNP_StatsData.csv\")\n",
    "\n",
    "#filtered data\n",
    "filterdCountryDf=filterCountryData(df,countries_chosen)\n",
    "\n",
    "dateDim = dateDimension()\n",
    "naturalDisasterDimension, nd_lookup=naturalDisasterDim(\n",
    "    dateDim,\n",
    "    countries_chosen=countries_chosen,\n",
    "    filePath=\"AssignmentData/ExternalSources/DISASTERS/1900_2021_DISASTERS.xlsx - emdat data.csv\"\n",
    ")\n",
    "\n",
    "# countryDimension = \n",
    "# display(naturalDisasterDimension.toPandas())\n",
    "# display(nd_lookup.toPandas())\n",
    "display(filterdCountryDf.toPandas())\n",
    "# display(dateDim.toPandas())\n",
    "filterdCountryDf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "aa8c6255",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SparkSession' object has no attribute 'parallelize'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [48]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m### TESTING BLOCK\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m \n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# test3.intersect(test2).show()\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallelize\u001b[49m([ \\\n\u001b[0;32m     18\u001b[0m \n\u001b[0;32m     19\u001b[0m     Row(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAlice\u001b[39m\u001b[38;5;124m'\u001b[39m, age\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m80\u001b[39m), \\\n\u001b[0;32m     20\u001b[0m \n\u001b[0;32m     21\u001b[0m     Row(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAlice\u001b[39m\u001b[38;5;124m'\u001b[39m, age\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m80\u001b[39m), \\\n\u001b[0;32m     22\u001b[0m \n\u001b[0;32m     23\u001b[0m     Row(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAlice\u001b[39m\u001b[38;5;124m'\u001b[39m, age\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m80\u001b[39m)])\u001b[38;5;241m.\u001b[39mtoDF()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'SparkSession' object has no attribute 'parallelize'"
     ]
    }
   ],
   "source": [
    "\n",
    "### TESTING BLOCK\n",
    "\n",
    "\n",
    "# ## LOOKUP TABLE LOGIC\n",
    "# # 2006-2010\n",
    "# tmp = dateDim.filter(fn.col(\"year\")==2006).select(fn.col(\"year\").alias(\"year_2\"))\n",
    "# dateDim_a = dateDim.alias(\"a\")\n",
    "# tmp_b = tmp.alias(\"b\")\n",
    "\n",
    "# test2 = dateDim_a.join(tmp_b.alias(\"b\"),tmp_b.year_2<dateDim_a.year)\n",
    "# test3 = dateDim_a.join(tmp_b.alias(\"b\"),2010>dateDim_a.year)\n",
    "\n",
    "# test2.show()\n",
    "# test3.show()\n",
    "\n",
    "# test3.intersect(test2).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "71b342e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST FUNCTIONS\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "\n",
    "# df - dataframe\n",
    "# col - column to observe\n",
    "def nullCount(df,cl):\n",
    "    non_null =(df\n",
    "     .filter(fn.col(cl).isNotNull())\n",
    "    )\n",
    "    \n",
    "    null = (df\n",
    "        .filter(fn.col(cl).isNull()))\n",
    "    \n",
    "    print(\"Number of non null values: \"+str(non_null.count()))\n",
    "    print(\"Number of null values: \"+str(null.count()))\n",
    "    \n",
    "    \n",
    "def summary_df(df,cl,bns = 10):\n",
    "    \"\"\"\n",
    "        returns null counts, basic statistics & plot of current values in a column\n",
    "        \n",
    "        df - dataframe you wish to observer these statistics\n",
    "        cl - column of which you wish to observe\n",
    "        bns - bins (number of bars) histogram will try to bucketize data in\n",
    "    \"\"\"\n",
    "    nullCount(df,cl)\n",
    "    \n",
    "#     df.groupBy(fn.col(cl)).count().orderBy(fn.asc(fn.col(cl))).show()\n",
    "#     df.groupBy(fn.col(cl)).count().orderBy(fn.desc(fn.col(cl))).show()\n",
    "    \n",
    "    tmp = df.filter(fn.col(cl).isNotNull())\n",
    "    tmp.select(cl).describe().show()\n",
    "\n",
    "    pd_data = tmp.select(fn.col(cl)).toPandas()\n",
    "    # display(pd_data)\n",
    "    plt.hist(pd_data,bins = bns)\n",
    "    plt.title(\"Histogram of \" +str(cl))\n",
    "    plt.xlabel(cl)\n",
    "    plt.ylabel(\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "12f4c8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of non null values: 144\n",
      "Number of null values: 0\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve '`net_migration`' given input columns: [country_key, country_name, year];\n'Project ['net_migration]\n+- Project [country_name#63837381, year#11767306, country_key#63837399L]\n   +- Filter isnotnull(net_migration#63837367)\n      +- Project [country_name#63837381, year#11767306, country_key#63837399L, net_migration#63837367]\n         +- Project [country_name#63837381, currency#63837380, region#63835462, population_total#63837360, population_growth#63837361, urban_population_growth#63837362, urban_population#63837363, rural_population#63837364, unemployment_rate#63837365, age_dependency_ratio_workingage#63836437, poverty_headcount_percentage#63837366, labor_force_total#63836898, net_migration#63837367, year#11767306, monotonically_increasing_id() AS country_key#63837399L]\n            +- Project [country_name#63837381, currency#63837380, region#63835462, population_total#63837360, population_growth#63837361, urban_population_growth#63837362, urban_population#63837363, rural_population#63837364, unemployment_rate#63837365, age_dependency_ratio_workingage#63836437, poverty_headcount_percentage#63837366, labor_force_total#63836898, net_migration#63837367, year#11767306]\n               +- Join Inner, (country_name#63837381 = country_name#63835517)\n                  :- Project [lower(Currency Unit#63835460) AS currency#63837380, lower(short name#63835456) AS country_name#63837381, region#63835462]\n                  :  +- Filter lower(short name#63835456) IN (united states,canada,mexico,thailand,finland,nigeria,somalia,norway,japan)\n                  :     +- Relation[Country Code#63835455,Short Name#63835456,Table Name#63835457,Long Name#63835458,2-alpha code#63835459,Currency Unit#63835460,Special Notes#63835461,Region#63835462,Income Group#63835463,WB-2 code#63835464,National accounts base year#63835465,National accounts reference year#63835466,SNA price valuation#63835467,Lending category#63835468,Other groups#63835469,System of National Accounts#63835470,Alternative conversion factor#63835471,PPP survey year#63835472,Balance of Payments Manual in use#63835473,External debt Reporting status#63835474,System of trade#63835475,Government Accounting concept#63835476,IMF data dissemination standard#63835477,Latest population census#63835478,... 7 more fields] csv\n                  +- Project [country_name#63835517, Population, total#11768599 AS population_total#63837360, Population growth (annual %)#11768594 AS population_growth#63837361, Urban population growth (annual %)#11768712 AS urban_population_growth#63837362, Urban population#11768710 AS urban_population#63837363, Rural population#11768668 AS rural_population#63837364, Unemployment, total (% of total labor force)#11768708 AS unemployment_rate#63837365, age_dependency_ratio_workingage#63836437, Poverty headcount ratio at national poverty line (% of population)#11768601 AS poverty_headcount_percentage#63837366, labor_force_total#63836898, Net migration#11768451 AS net_migration#63837367, year#11767306]\n                     +- Project [Year#11767306, AIDS estimated deaths (UNAIDS estimates)#11768264, ARI treatment (% of children under 5 taken to a health provider)#11768265, Adolescent fertility rate (births per 1,000 women ages 15-19)#11768266, Adults (ages 15+) and children (0-14 years) living with HIV#11768267, Adults (ages 15+) and children (ages 0-14) newly infected with HIV#11768268, Adults (ages 15+) living with HIV#11768269, Adults (ages 15-49) newly infected with HIV#11768270, Age at first marriage, female#11768271, Age at first marriage, male#11768272, Age dependency ratio (% of working-age population)#11768273, Age dependency ratio, old#11768274, Age dependency ratio, young#11768275, Age population, age 00, female, interpolated#11768276, Age population, age 00, male, interpolated#11768277, Age population, age 01, female, interpolated#11768278, Age population, age 01, male, interpolated#11768279, Age population, age 02, female, interpolated#11768280, Age population, age 02, male, interpolated#11768281, Age population, age 03, female, interpolated#11768282, Age population, age 03, male, interpolated#11768283, Age population, age 04, female, interpolated#11768284, Age population, age 04, male, interpolated#11768285, Age population, age 05, female, interpolated#11768286, ... 437 more fields]\n                        +- Project [Year#11767306, AIDS estimated deaths (UNAIDS estimates)#11768264, ARI treatment (% of children under 5 taken to a health provider)#11768265, Adolescent fertility rate (births per 1,000 women ages 15-19)#11768266, Adults (ages 15+) and children (0-14 years) living with HIV#11768267, Adults (ages 15+) and children (ages 0-14) newly infected with HIV#11768268, Adults (ages 15+) living with HIV#11768269, Adults (ages 15-49) newly infected with HIV#11768270, Age at first marriage, female#11768271, Age at first marriage, male#11768272, Age dependency ratio (% of working-age population)#11768273, Age dependency ratio, old#11768274, Age dependency ratio, young#11768275, Age population, age 00, female, interpolated#11768276, Age population, age 00, male, interpolated#11768277, Age population, age 01, female, interpolated#11768278, Age population, age 01, male, interpolated#11768279, Age population, age 02, female, interpolated#11768280, Age population, age 02, male, interpolated#11768281, Age population, age 03, female, interpolated#11768282, Age population, age 03, male, interpolated#11768283, Age population, age 04, female, interpolated#11768284, Age population, age 04, male, interpolated#11768285, Age population, age 05, female, interpolated#11768286, ... 436 more fields]\n                           +- Project [Year#11767306, AIDS estimated deaths (UNAIDS estimates)#11768264, ARI treatment (% of children under 5 taken to a health provider)#11768265, Adolescent fertility rate (births per 1,000 women ages 15-19)#11768266, Adults (ages 15+) and children (0-14 years) living with HIV#11768267, Adults (ages 15+) and children (ages 0-14) newly infected with HIV#11768268, Adults (ages 15+) living with HIV#11768269, Adults (ages 15-49) newly infected with HIV#11768270, Age at first marriage, female#11768271, Age at first marriage, male#11768272, Age dependency ratio (% of working-age population)#11768273, Age dependency ratio, old#11768274, Age dependency ratio, young#11768275, Age population, age 00, female, interpolated#11768276, Age population, age 00, male, interpolated#11768277, Age population, age 01, female, interpolated#11768278, Age population, age 01, male, interpolated#11768279, Age population, age 02, female, interpolated#11768280, Age population, age 02, male, interpolated#11768281, Age population, age 03, female, interpolated#11768282, Age population, age 03, male, interpolated#11768283, Age population, age 04, female, interpolated#11768284, Age population, age 04, male, interpolated#11768285, Age population, age 05, female, interpolated#11768286, ... 435 more fields]\n                              +- Project [Country Name#11767071, Year#11767306, AIDS estimated deaths (UNAIDS estimates)#11768264, ARI treatment (% of children under 5 taken to a health provider)#11768265, Adolescent fertility rate (births per 1,000 women ages 15-19)#11768266, Adults (ages 15+) and children (0-14 years) living with HIV#11768267, Adults (ages 15+) and children (ages 0-14) newly infected with HIV#11768268, Adults (ages 15+) living with HIV#11768269, Adults (ages 15-49) newly infected with HIV#11768270, Age at first marriage, female#11768271, Age at first marriage, male#11768272, Age dependency ratio (% of working-age population)#11768273, Age dependency ratio, old#11768274, Age dependency ratio, young#11768275, Age population, age 00, female, interpolated#11768276, Age population, age 00, male, interpolated#11768277, Age population, age 01, female, interpolated#11768278, Age population, age 01, male, interpolated#11768279, Age population, age 02, female, interpolated#11768280, Age population, age 02, male, interpolated#11768281, Age population, age 03, female, interpolated#11768282, Age population, age 03, male, interpolated#11768283, Age population, age 04, female, interpolated#11768284, Age population, age 04, male, interpolated#11768285, ... 436 more fields]\n                                 +- Project [Country Name#11767071, Year#11767306, __pivot_sum(`Value`) AS `sum(``Value``)`#11768263[0] AS AIDS estimated deaths (UNAIDS estimates)#11768264, __pivot_sum(`Value`) AS `sum(``Value``)`#11768263[1] AS ARI treatment (% of children under 5 taken to a health provider)#11768265, __pivot_sum(`Value`) AS `sum(``Value``)`#11768263[2] AS Adolescent fertility rate (births per 1,000 women ages 15-19)#11768266, __pivot_sum(`Value`) AS `sum(``Value``)`#11768263[3] AS Adults (ages 15+) and children (0-14 years) living with HIV#11768267, __pivot_sum(`Value`) AS `sum(``Value``)`#11768263[4] AS Adults (ages 15+) and children (ages 0-14) newly infected with HIV#11768268, __pivot_sum(`Value`) AS `sum(``Value``)`#11768263[5] AS Adults (ages 15+) living with HIV#11768269, __pivot_sum(`Value`) AS `sum(``Value``)`#11768263[6] AS Adults (ages 15-49) newly infected with HIV#11768270, __pivot_sum(`Value`) AS `sum(``Value``)`#11768263[7] AS Age at first marriage, female#11768271, __pivot_sum(`Value`) AS `sum(``Value``)`#11768263[8] AS Age at first marriage, male#11768272, __pivot_sum(`Value`) AS `sum(``Value``)`#11768263[9] AS Age dependency ratio (% of working-age population)#11768273, __pivot_sum(`Value`) AS `sum(``Value``)`#11768263[10] AS Age dependency ratio, old#11768274, __pivot_sum(`Value`) AS `sum(``Value``)`#11768263[11] AS Age dependency ratio, young#11768275, __pivot_sum(`Value`) AS `sum(``Value``)`#11768263[12] AS Age population, age 00, female, interpolated#11768276, __pivot_sum(`Value`) AS `sum(``Value``)`#11768263[13] AS Age population, age 00, male, interpolated#11768277, __pivot_sum(`Value`) AS `sum(``Value``)`#11768263[14] AS Age population, age 01, female, interpolated#11768278, __pivot_sum(`Value`) AS `sum(``Value``)`#11768263[15] AS Age population, age 01, male, interpolated#11768279, __pivot_sum(`Value`) AS `sum(``Value``)`#11768263[16] AS Age population, age 02, female, interpolated#11768280, __pivot_sum(`Value`) AS `sum(``Value``)`#11768263[17] AS Age population, age 02, male, interpolated#11768281, __pivot_sum(`Value`) AS `sum(``Value``)`#11768263[18] AS Age population, age 03, female, interpolated#11768282, __pivot_sum(`Value`) AS `sum(``Value``)`#11768263[19] AS Age population, age 03, male, interpolated#11768283, __pivot_sum(`Value`) AS `sum(``Value``)`#11768263[20] AS Age population, age 04, female, interpolated#11768284, __pivot_sum(`Value`) AS `sum(``Value``)`#11768263[21] AS Age population, age 04, male, interpolated#11768285, ... 435 more fields]\n                                    +- Aggregate [Country Name#11767071, Year#11767306], [Country Name#11767071, Year#11767306, pivotfirst(Indicator Name#11767073, sum(`Value`)#11767347, AIDS estimated deaths (UNAIDS estimates), ARI treatment (% of children under 5 taken to a health provider), Adolescent fertility rate (births per 1,000 women ages 15-19), Adults (ages 15+) and children (0-14 years) living with HIV, Adults (ages 15+) and children (ages 0-14) newly infected with HIV, Adults (ages 15+) living with HIV, Adults (ages 15-49) newly infected with HIV, Age at first marriage, female, Age at first marriage, male, Age dependency ratio (% of working-age population), Age dependency ratio, old, Age dependency ratio, young, Age population, age 00, female, interpolated, Age population, age 00, male, interpolated, Age population, age 01, female, interpolated, Age population, age 01, male, interpolated, Age population, age 02, female, interpolated, Age population, age 02, male, interpolated, Age population, age 03, female, interpolated, Age population, age 03, male, interpolated, Age population, age 04, female, interpolated, Age population, age 04, male, interpolated, Age population, age 05, female, interpolated, Age population, age 05, male, interpolated, Age population, age 06, female, interpolated, Age population, age 06, male, interpolated, Age population, age 07, female, interpolated, Age population, age 07, male, interpolated, Age population, age 08, female, interpolated, Age population, age 08, male, interpolated, Age population, age 09, female, interpolated, Age population, age 09, male, interpolated, Age population, age 10, female, interpolated, Age population, age 10, male, interpolated, Age population, age 11, female, interpolated, Age population, age 11, male, interpolated, Age population, age 12, female, interpolated, Age population, age 12, male, interpolated, Age population, age 13, female, interpolated, Age population, age 13, male, interpolated, Age population, age 14, female, interpolated, Age population, age 14, male, interpolated, Age population, age 15, female, interpolated, Age population, age 15, male, interpolated, Age population, age 16, female, interpolated, Age population, age 16, male, interpolated, Age population, age 17, female, interpolated, Age population, age 17, male, interpolated, Age population, age 18, female, interpolated, Age population, age 18, male, interpolated, Age population, age 19, female, interpolated, Age population, age 19, male, interpolated, Age population, age 20, female, interpolated, Age population, age 20, male, interpolated, Age population, age 21, female, interpolated, Age population, age 21, male, interpolated, Age population, age 22, female, interpolated, Age population, age 22, male, interpolated, Age population, age 23, female, interpolated, Age population, age 23, male, interpolated, Age population, age 24, female, interpolated, Age population, age 24, male, interpolated, Age population, age 25, female, interpolated, Age population, age 25, male, interpolated, Antiretroviral therapy coverage (% of people living with HIV), Antiretroviral therapy coverage for PMTCT (% of pregnant women living with HIV), Birth rate, crude (per 1,000 people), Births attended by skilled health staff (% of total), Capital health expenditure (% of GDP), Cause of death, by communicable diseases and maternal, prenatal and nutrition conditions (% of total), Cause of death, by injury (% of total), Cause of death, by non-communicable diseases (% of total), Children (0-14) living with HIV, Children (ages 0-14) newly infected with HIV, Children orphaned by HIV/AIDS, Children with fever receiving antimalarial drugs (% of children under age 5 with fever), Community health workers (per 1,000 people), Completeness of birth registration (%), Completeness of birth registration, female (%), Completeness of birth registration, male (%), Completeness of birth registration, rural (%), Completeness of birth registration, urban (%), Completeness of death registration with cause-of-death information (%), Comprehensive correct knowledge of HIV/AIDS, ages 15-24, female (2 prevent ways and reject 3 misconceptions), Comprehensive correct knowledge of HIV/AIDS, ages 15-24, male (2 prevent ways and reject 3 misconceptions), Comprehensive correct knowledge of HIV/AIDS, ages 15-49, female (2 prevent ways and reject 3 misconceptions), Comprehensive correct knowledge of HIV/AIDS, ages 15-49, male (2 prevent ways and reject 3 misconceptions), Condom use at last high-risk sex, adult female (% ages 15-49), Condom use at last high-risk sex, adult male (% ages 15-49), Condom use, population ages 15-24, female (% of females ages 15-24), Condom use, population ages 15-24, male (% of males ages 15-24), Consumption of iodized salt (% of households), Contraceptive prevalence, any method (% of married women ages 15-49), Contraceptive prevalence, any method (% of sexually active unmarried women ages 15-49), Contraceptive prevalence, any modern method (% of married women ages 15-49), Contraceptive prevalence, any modern method (% of sexually active unmarried women ages 15-49), Current health expenditure (% of GDP), Current health expenditure per capita (current US$), Current health expenditure per capita, PPP (current international $), Death rate, crude (per 1,000 people), Demand for family planning satisfied by any methods (% of married women with demand for family planning), Demand for family planning satisfied by modern methods (% of married women with demand for family planning), Diabetes prevalence (% of population ages 20 to 79), Diarrhea treatment (% of children under 5 receiving oral rehydration and continued feeding), Diarrhea treatment (% of children under 5 who received ORS packet), Domestic general government health expenditure (% of GDP), Domestic general government health expenditure (% of current health expenditure), Domestic general government health expenditure (% of general government expenditure), Domestic general government health expenditure per capita (current US$), Domestic general government health expenditure per capita, PPP (current international $), Domestic private health expenditure (% of current health expenditure), Domestic private health expenditure per capita (current US$), Domestic private health expenditure per capita, PPP  (current international $), Exclusive breastfeeding (% of children under 6 months), External health expenditure (% of current health expenditure), External health expenditure channeled through government (% of external health expenditure), External health expenditure per capita (current US$), External health expenditure per capita, PPP (current international $), Female headed households (% of households with a female head), Fertility rate, total (births per woman), GNI per capita, Atlas method (current US$), Hospital beds (per 1,000 people), Human capital index (HCI) (scale 0-1), Human capital index (HCI), female (scale 0-1), Human capital index (HCI), female, lower bound (scale 0-1), Human capital index (HCI), female, upper bound (scale 0-1), Human capital index (HCI), lower bound (scale 0-1), Human capital index (HCI), male (scale 0-1), Human capital index (HCI), male, lower bound (scale 0-1), Human capital index (HCI), male, upper bound (scale 0-1), Human capital index (HCI), upper bound (scale 0-1), Immunization, BCG (% of one-year-old children), Immunization, DPT (% of children ages 12-23 months), Immunization, HepB3 (% of one-year-old children), Immunization, Hib3 (% of children ages 12-23 months), Immunization, Pol3 (% of one-year-old children), Immunization, measles (% of children ages 12-23 months), Immunization, measles second dose (% of children by the nationally recommended age), Incidence of HIV, ages 15-24 (per 1,000 uninfected population ages 15-24), Incidence of HIV, ages 15-49 (per 1,000 uninfected population ages 15-49), Incidence of HIV, ages 50+ (per 1,000 uninfected population ages 50+), Incidence of HIV, all (per 1,000 uninfected population), Incidence of malaria (per 1,000 population at risk), Incidence of tuberculosis (per 100,000 people), Infant and young child feeding practices, all 3 IYCF (% children ages 6-23 months), Intermittent preventive treatment (IPT) of malaria in pregnancy (% of pregnant women), Labor force, female (% of total labor force), Labor force, total, Life expectancy at birth, female (years), Life expectancy at birth, male (years), Life expectancy at birth, total (years), Lifetime risk of maternal death (%), Lifetime risk of maternal death (1 in: rate varies by country), Literacy rate, adult female (% of females ages 15 and above), Literacy rate, adult male (% of males ages 15 and above), Literacy rate, adult total (% of people ages 15 and above), Literacy rate, youth male (% of males ages 15-24), Literacy rate, youth total (% of people ages 15-24), Low-birthweight babies (% of births), Malaria cases reported, Maternal leave benefits (% of wages paid in covered period), Maternal mortality ratio (modeled estimate, per 100,000 live births), Maternal mortality ratio (national estimate, per 100,000 live births), Mortality caused by road traffic injury (per 100,000 people), Mortality caused by road traffic injury, female (per 100,000 female population), Mortality caused by road traffic injury, male (per 100,000 male population), Mortality from CVD, cancer, diabetes or CRD between exact ages 30 and 70 (%), Mortality from CVD, cancer, diabetes or CRD between exact ages 30 and 70, female (%), Mortality from CVD, cancer, diabetes or CRD between exact ages 30 and 70, male (%), Mortality rate attributed to household and ambient air pollution (per 100,000 population), Mortality rate attributed to household and ambient air pollution, age-standardized, female (per 100,000 female population), Mortality rate attributed to household and ambient air pollution, age-standardized, male (per 100,000 male population), Mortality rate attributed to unintentional poisoning (per 100,000 population), Mortality rate attributed to unintentional poisoning, female (per 100,000 female population), Mortality rate attributed to unintentional poisoning, male (per 100,000 male population), Mortality rate attributed to unsafe water, unsafe sanitation and lack of hygiene (per 100,000 population), Mortality rate attributed to unsafe water, unsafe sanitation and lack of hygiene, female (per 100,000 female population), Mortality rate attributed to unsafe water, unsafe sanitation and lack of hygiene, male (per 100,000 male population), Mortality rate, adult, female (per 1,000 female adults), Mortality rate, adult, male (per 1,000 male adults), Mortality rate, infant (per 1,000 live births), Mortality rate, infant, female (per 1,000 live births), Mortality rate, infant, male (per 1,000 live births), Mortality rate, neonatal (per 1,000 live births), Mortality rate, under-5 (per 1,000), Mortality rate, under-5, female (per 1,000), Mortality rate, under-5, male (per 1,000), Net migration, Newborns protected against tetanus (%), Number of deaths ages 10-14 years, Number of deaths ages 10-14 years, female, Number of deaths ages 10-14 years, male, Number of deaths ages 10-19 years, Number of deaths ages 10-19 years, female, Number of deaths ages 10-19 years, male, Number of deaths ages 15-19 years, Number of deaths ages 15-19 years, female, Number of deaths ages 15-19 years, male, Number of deaths ages 20-24 years, Number of deaths ages 20-24 years, female, Number of deaths ages 20-24 years, male, Number of deaths ages 5-9 years, Number of deaths ages 5-9 years, female, Number of deaths ages 5-9 years, male, Number of infant deaths, Number of infant deaths, female, Number of infant deaths, male, Number of maternal deaths, Number of neonatal deaths, Number of people pushed below the $1.90 ($ 2011 PPP) poverty line by out-of-pocket health care expenditure, Number of people pushed below the $3.20 ($ 2011 PPP) poverty line by out-of-pocket health care expenditure, Number of people pushed further below the $1.90 ($ 2011 PPP) poverty line by out-of-pocket health care expenditure, Number of people pushed further below the $3.20 ($ 2011 PPP) poverty line by out-of-pocket health care expenditure, Number of people spending more than 10% of household consumption or income on out-of-pocket health care expenditure, Number of people spending more than 25% of household consumption or income on out-of-pocket health care expenditure, Number of people who are undernourished, Number of stillbirths, Number of surgical procedures (per 100,000 population), Number of under-five deaths, Number of under-five deaths, female, Number of under-five deaths, male, Nurses and midwives (per 1,000 people), Out-of-pocket expenditure (% of current health expenditure), Out-of-pocket expenditure per capita (current US$), Out-of-pocket expenditure per capita, PPP (current international $), People practicing open defecation (% of population), People practicing open defecation, rural (% of rural population), People practicing open defecation, urban (% of urban population), People using at least basic drinking water services (% of population), People using at least basic drinking water services, rural (% of rural population), People using at least basic drinking water services, urban (% of urban population), People using at least basic sanitation services (% of population), People using at least basic sanitation services, rural (% of rural population), People using at least basic sanitation services, urban  (% of urban population), People using safely managed drinking water services (% of population), People using safely managed drinking water services, rural (% of rural population), People using safely managed drinking water services, urban (% of urban population), People using safely managed sanitation services (% of population), People using safely managed sanitation services, rural (% of rural population), People using safely managed sanitation services, urban  (% of urban population), People with basic handwashing facilities including soap and water (% of population), People with basic handwashing facilities including soap and water, rural (% of rural population), People with basic handwashing facilities including soap and water, urban (% of urban population), Physicians (per 1,000 people), Population ages 0-14 (% of total population), Population ages 0-14, female, Population ages 0-14, female (% of female population), Population ages 0-14, male, Population ages 0-14, male (% of male population), Population ages 00-04, female, Population ages 00-04, female (% of female population), Population ages 00-04, male, Population ages 00-04, male (% of male population), Population ages 00-14, total, Population ages 05-09, female, Population ages 05-09, female (% of female population), Population ages 05-09, male, Population ages 05-09, male (% of male population), Population ages 10-14, female, Population ages 10-14, female (% of female population), Population ages 10-14, male, Population ages 10-14, male (% of male population), Population ages 15-19, female, Population ages 15-19, female (% of female population), Population ages 15-19, male, Population ages 15-19, male (% of male population), Population ages 15-64 (% of total population), Population ages 15-64, female, Population ages 15-64, female (% of female population), Population ages 15-64, male, Population ages 15-64, male (% of male population), Population ages 15-64, total, Population ages 20-24, female, Population ages 20-24, female (% of female population), Population ages 20-24, male, Population ages 20-24, male (% of male population), Population ages 25-29, female, Population ages 25-29, female (% of female population), Population ages 25-29, male, Population ages 25-29, male (% of male population), Population ages 30-34, female, Population ages 30-34, female (% of female population), Population ages 30-34, male, Population ages 30-34, male (% of male population), Population ages 35-39, female, Population ages 35-39, female (% of female population), Population ages 35-39, male, Population ages 35-39, male (% of male population), Population ages 40-44, female, Population ages 40-44, female (% of female population), Population ages 40-44, male, Population ages 40-44, male (% of male population), Population ages 45-49, female, Population ages 45-49, female (% of female population), Population ages 45-49, male, Population ages 45-49, male (% of male population), Population ages 50-54, female, Population ages 50-54, female (% of female population), Population ages 50-54, male, Population ages 50-54, male (% of male population), Population ages 55-59, female, Population ages 55-59, female (% of female population), Population ages 55-59, male, Population ages 55-59, male (% of male population), Population ages 60-64, female, Population ages 60-64, female (% of female population), Population ages 60-64, male, Population ages 60-64, male (% of male population), Population ages 65 and above (% of total population), Population ages 65 and above, female, Population ages 65 and above, female (% of female population), Population ages 65 and above, male, Population ages 65 and above, male (% of male population), Population ages 65 and above, total, Population ages 65-69, female, Population ages 65-69, female (% of female population), Population ages 65-69, male, Population ages 65-69, male (% of male population), Population ages 70-74, female, Population ages 70-74, female (% of female population), Population ages 70-74, male, Population ages 70-74, male (% of male population), Population ages 75-79, female, Population ages 75-79, female (% of female population), Population ages 75-79, male, Population ages 75-79, male (% of male population), Population ages 80 and above, female, Population ages 80 and above, male, Population ages 80 and above, male (% of male population), Population ages 80 and older, female (% of female population), Population growth (annual %), Population, female, Population, female (% of total population), Population, male, Population, male (% of total population), Population, total, Postnatal care coverage (% mothers), Poverty headcount ratio at national poverty line (% of population), Pregnant women receiving prenatal care (%), Pregnant women receiving prenatal care of at least four visits (% of pregnant women), Prevalence of HIV, female (% ages 15-24), Prevalence of HIV, male (% ages 15-24), Prevalence of HIV, total (% of population ages 15-49), Prevalence of anemia among children (% of children ages 6-59 months), Prevalence of anemia among non-pregnant women (% of women ages 15-49), Prevalence of anemia among pregnant women (%), Prevalence of anemia among women of reproductive age (% of women ages 15-49), Prevalence of current tobacco use (% of adults), Prevalence of current tobacco use, females (% of female adults), Prevalence of current tobacco use, males (% of male adults), Prevalence of hypertension (% of adults ages 30-79), Prevalence of hypertension, female (% of female adults ages 30-79), Prevalence of hypertension, male (% of male adults ages 30-79), Prevalence of overweight (% of adults), Prevalence of overweight (% of children under 5), Prevalence of overweight (modeled estimate, % of children under 5), Prevalence of overweight, female (% of children under 5), Prevalence of overweight, female (% of female adults), Prevalence of overweight, male (% of children under 5), Prevalence of overweight, male (% of male adults), Prevalence of severe wasting, weight for height (% of children under 5), Prevalence of severe wasting, weight for height, female (% of children under 5), Prevalence of severe wasting, weight for height, male (% of children under 5), Prevalence of stunting, height for age (% of children under 5), Prevalence of stunting, height for age (modeled estimate, % of children under 5), Prevalence of stunting, height for age, female (% of children under 5), Prevalence of stunting, height for age, male (% of children under 5), Prevalence of syphilis (% of women attending antenatal care), Prevalence of undernourishment (% of population), Prevalence of underweight, weight for age (% of children under 5), Prevalence of underweight, weight for age, female (% of children under 5), Prevalence of underweight, weight for age, male (% of children under 5), Prevalence of wasting, weight for height (% of children under 5), Prevalence of wasting, weight for height, female (% of children under 5), Prevalence of wasting, weight for height, male (% of children under 5), Primary completion rate, female (% of relevant age group), Primary completion rate, male (% of relevant age group), Primary completion rate, total (% of relevant age group), Probability of dying among adolescents ages 10-14 years (per 1,000), Probability of dying among adolescents ages 10-14 years, female (per 1,000), Probability of dying among adolescents ages 10-14 years, male (per 1,000), Probability of dying among adolescents ages 10-19 years (per 1,000), Probability of dying among adolescents ages 10-19 years, female (per 1,000), Probability of dying among adolescents ages 10-19 years, male (per 1,000), Probability of dying among adolescents ages 15-19 years (per 1,000), Probability of dying among adolescents ages 15-19 years, female (per 1,000), Probability of dying among adolescents ages 15-19 years, male (per 1,000), Probability of dying among children ages 5-9 years (per 1,000), Probability of dying among children ages 5-9 years, female (per 1,000), Probability of dying among children ages 5-9 years, male (per 1,000), Probability of dying among youth ages 20-24 years (per 1,000), Probability of dying among youth ages 20-24 years, female (per 1,000), Probability of dying among youth ages 20-24 years, male (per 1,000), Proportion of population pushed below the $1.90 ($ 2011 PPP) poverty line by out-of-pocket health care expenditure (%), Proportion of population pushed below the $3.20 ($ 2011 PPP) poverty line by out-of-pocket health care expenditure (%), Proportion of population pushed further below the $1.90 ($ 2011 PPP) poverty line by out-of-pocket health care expenditure (%), Proportion of population pushed further below the $3.20 ($ 2011 PPP) poverty line by out-of-pocket health care expenditure (%), Proportion of population spending more than 10% of household consumption or income on out-of-pocket health care expenditure (%), Proportion of population spending more than 25% of household consumption or income on out-of-pocket health care expenditure (%), Public spending on education, total (% of GDP), Ratio of school attendance of orphans to school attendance of non-orphans ages 10-14, Ratio of young literate females to males (% ages 15-24), Risk of catastrophic expenditure for surgical care (% of people at risk), Risk of impoverishing expenditure for surgical care (% of people at risk), Rural population, Rural population (% of total population), Rural population growth (annual %), Rural poverty headcount ratio at national poverty lines (% of rural population), School enrollment, primary (% gross), School enrollment, primary (% net), School enrollment, primary, female (% gross), School enrollment, primary, female (% net), School enrollment, primary, male (% gross), School enrollment, primary, male (% net), School enrollment, secondary (% gross), School enrollment, secondary (% net), School enrollment, secondary, female (% gross), School enrollment, secondary, female (% net), School enrollment, secondary, male (% gross), School enrollment, secondary, male (% net), School enrollment, tertiary (% gross), School enrollment, tertiary, female (% gross), Sex ratio at birth (male births per female births), Share of women employed in the nonagricultural sector (% of total nonagricultural employment), Specialist surgical workforce (per 100,000 population), Stillbirth rate (per 1,000 total births), Suicide mortality rate (per 100,000 population), Suicide mortality rate, female (per 100,000 female population), Suicide mortality rate, male (per 100,000 male population), Survival to age 65, female (% of cohort), Survival to age 65, male (% of cohort), Teenage mothers (% of women ages 15-19 who have had children or are currently pregnant), Total alcohol consumption per capita (liters of pure alcohol, projected estimates, 15+ years of age), Total alcohol consumption per capita, female (liters of pure alcohol, projected estimates, female 15+ years of age), Total alcohol consumption per capita, male (liters of pure alcohol, projected estimates, male 15+ years of age), Treatment for hypertension (% of adults ages 30-79 with hypertension), Treatment for hypertension, female (% of female adults ages 30-79 with hypertension), Treatment for hypertension, male (% of male adults ages 30-79 with hypertension), Tuberculosis case detection rate (%, all forms), Tuberculosis death rate (per 100,000 people), Tuberculosis treatment success rate (% of new cases), UHC service coverage index, Unemployment, female (% of female labor force), Unemployment, male (% of male labor force), Unemployment, total (% of total labor force), Unmet need for contraception (% of married women ages 15-49), Urban population, Urban population (% of total population), Urban population growth (annual %), Urban poverty headcount ratio at national poverty lines (% of urban population), Use of insecticide-treated bed nets (% of under-5 population), Vitamin A supplementation coverage rate (% of children ages 6-59 months), Wanted fertility rate (births per woman), Women who were first married by age 15 (% of women ages 20-24), Women who were first married by age 18 (% of women ages 20-24), Women's share of population ages 15+ living with HIV (%), Young people (ages 15-24) newly infected with HIV, 0, 0) AS __pivot_sum(`Value`) AS `sum(``Value``)`#11768263]\n                                       +- Aggregate [Country Name#11767071, Year#11767306, Indicator Name#11767073], [Country Name#11767071, Year#11767306, Indicator Name#11767073, sum(Value#11767307) AS sum(`Value`)#11767347]\n                                          +- Project [Country Name#11767071, Country Code#11767072, Indicator Name#11767073, Year#11767306, Value#11767307, month(cast(date#11767281 as date)) AS month#11767303, dayofmonth(cast(date#11767281 as date)) AS day#11767304, quarter(cast(date#11767281 as date)) AS quarter#11767305]\n                                             +- Generate stack(16, 2005, 2005#11767245, 2006, 2006#11767246, 2007, 2007#11767247, 2008, 2008#11767248, 2009, 2009#11767249, 2010, 2010#11767250, 2011, 2011#11767251, 2012, 2012#11767252, 2013, 2013#11767253, 2014, 2014#11767254, 2015, 2015#11767255, 2016, ... 9 more fields), false, [Year#11767306, Value#11767307]\n                                                +- Project [Country Name#11767071, Country Code#11767072, Indicator Name#11767073, Indicator Code#11767074, 2005#11767245, 2006#11767246, 2007#11767247, 2008#11767248, 2009#11767249, 2010#11767250, 2011#11767251, 2012#11767252, 2013#11767253, 2014#11767254, 2015#11767255, 2016#11767256, 2017#11767257, 2018#11767258, 2019#11767259, 2020#11767260, date_format(cast(2020-04-01 as timestamp), yyyy-MM-dd, Some(America/Toronto)) AS date#11767281]\n                                                   +- Project [Country Name#11767071, Country Code#11767072, Indicator Name#11767073, Indicator Code#11767074, coalesce(nanvl(2005#11767120, cast(null as double)), cast(0.0 as double)) AS 2005#11767245, coalesce(nanvl(2006#11767121, cast(null as double)), cast(0.0 as double)) AS 2006#11767246, coalesce(nanvl(2007#11767122, cast(null as double)), cast(0.0 as double)) AS 2007#11767247, coalesce(nanvl(2008#11767123, cast(null as double)), cast(0.0 as double)) AS 2008#11767248, coalesce(nanvl(2009#11767124, cast(null as double)), cast(0.0 as double)) AS 2009#11767249, coalesce(nanvl(2010#11767125, cast(null as double)), cast(0.0 as double)) AS 2010#11767250, coalesce(nanvl(2011#11767126, cast(null as double)), cast(0.0 as double)) AS 2011#11767251, coalesce(nanvl(2012#11767127, cast(null as double)), cast(0.0 as double)) AS 2012#11767252, coalesce(nanvl(2013#11767128, cast(null as double)), cast(0.0 as double)) AS 2013#11767253, coalesce(nanvl(2014#11767129, cast(null as double)), cast(0.0 as double)) AS 2014#11767254, coalesce(nanvl(2015#11767130, cast(null as double)), cast(0.0 as double)) AS 2015#11767255, coalesce(nanvl(2016#11767131, cast(null as double)), cast(0.0 as double)) AS 2016#11767256, coalesce(nanvl(2017#11767132, cast(null as double)), cast(0.0 as double)) AS 2017#11767257, coalesce(nanvl(2018#11767133, cast(null as double)), cast(0.0 as double)) AS 2018#11767258, coalesce(nanvl(2019#11767134, cast(null as double)), cast(0.0 as double)) AS 2019#11767259, coalesce(nanvl(2020#11767135, cast(null as double)), cast(0.0 as double)) AS 2020#11767260]\n                                                      +- Filter Country Name#11767071 IN (United States,Canada,Mexico,Thailand,Finland,Nigeria,Somalia,Norway,Japan)\n                                                         +- Project [Country Name#11767071, Country Code#11767072, Indicator Name#11767073, Indicator Code#11767074, 2005#11767120, 2006#11767121, 2007#11767122, 2008#11767123, 2009#11767124, 2010#11767125, 2011#11767126, 2012#11767127, 2013#11767128, 2014#11767129, 2015#11767130, 2016#11767131, 2017#11767132, 2018#11767133, 2019#11767134, 2020#11767135]\n                                                            +- Relation[Country Name#11767071,Country Code#11767072,Indicator Name#11767073,Indicator Code#11767074,1960#11767075,1961#11767076,1962#11767077,1963#11767078,1964#11767079,1965#11767080,1966#11767081,1967#11767082,1968#11767083,1969#11767084,1970#11767085,1971#11767086,1972#11767087,1973#11767088,1974#11767089,1975#11767090,1976#11767091,1977#11767092,1978#11767093,1979#11767094,... 43 more fields] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [115]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#\"Total Damages ('000 US$)\"\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43msummary_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtmp\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnet_migration\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mbns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [66]\u001b[0m, in \u001b[0;36msummary_df\u001b[1;34m(df, cl, bns)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m#     df.groupBy(fn.col(cl)).count().orderBy(fn.asc(fn.col(cl))).show()\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m#     df.groupBy(fn.col(cl)).count().orderBy(fn.desc(fn.col(cl))).show()\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     tmp \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mfilter(fn\u001b[38;5;241m.\u001b[39mcol(cl)\u001b[38;5;241m.\u001b[39misNotNull())\n\u001b[1;32m---> 35\u001b[0m     \u001b[43mtmp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcl\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdescribe()\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m     37\u001b[0m     pd_data \u001b[38;5;241m=\u001b[39m tmp\u001b[38;5;241m.\u001b[39mselect(fn\u001b[38;5;241m.\u001b[39mcol(cl))\u001b[38;5;241m.\u001b[39mtoPandas()\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;66;03m# display(pd_data)\u001b[39;00m\n",
      "File \u001b[1;32m~\\venv\\lib\\site-packages\\pyspark\\sql\\dataframe.py:1685\u001b[0m, in \u001b[0;36mDataFrame.select\u001b[1;34m(self, *cols)\u001b[0m\n\u001b[0;32m   1664\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcols):\n\u001b[0;32m   1665\u001b[0m     \u001b[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[0;32m   1666\u001b[0m \n\u001b[0;32m   1667\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1683\u001b[0m \u001b[38;5;124;03m    [Row(name='Alice', age=12), Row(name='Bob', age=15)]\u001b[39;00m\n\u001b[0;32m   1684\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1685\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jcols\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1686\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msql_ctx)\n",
      "File \u001b[1;32m~\\venv\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32m~\\venv\\lib\\site-packages\\pyspark\\sql\\utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: cannot resolve '`net_migration`' given input columns: [country_key, country_name, year];\n'Project ['net_migration]\n+- Project [country_name#63837381, year#11767306, country_key#63837399L]\n   +- Filter isnotnull(net_migration#63837367)\n      +- Project [country_name#63837381, year#11767306, country_key#63837399L, net_migration#63837367]\n         +- Project [country_name#63837381, currency#63837380, region#63835462, population_total#63837360, population_growth#63837361, urban_population_growth#63837362, urban_population#63837363, rural_population#63837364, unemployment_rate#63837365, age_dependency_ratio_workingage#63836437, poverty_headcount_percentage#63837366, labor_force_total#63836898, net_migration#63837367, year#11767306, monotonically_increasing_id() AS country_key#63837399L]\n            +- Project [country_name#63837381, currency#63837380, region#63835462, population_total#63837360, population_growth#63837361, urban_population_growth#63837362, urban_population#63837363, rural_population#63837364, unemployment_rate#63837365, age_dependency_ratio_workingage#63836437, poverty_headcount_percentage#63837366, labor_force_total#63836898, net_migration#63837367, year#11767306]\n               +- Join Inner, (country_name#63837381 = country_name#63835517)\n                  :- Project [lower(Currency Unit#63835460) AS currency#63837380, lower(short name#63835456) AS country_name#63837381, region#63835462]\n                  :  +- Filter lower(short name#63835456) IN (united states,canada,mexico,thailand,finland,nigeria,somalia,norway,japan)\n                  :     +- Relation[Country Code#63835455,Short Name#63835456,Table Name#63835457,Long Name#63835458,2-alpha code#63835459,Currency Unit#63835460,Special Notes#63835461,Region#63835462,Income Group#63835463,WB-2 code#63835464,National accounts base year#63835465,National accounts reference year#63835466,SNA price valuation#63835467,Lending category#63835468,Other groups#63835469,System of National Accounts#63835470,Alternative conversion factor#63835471,PPP survey year#63835472,Balance of Payments Manual in use#63835473,External debt Reporting status#63835474,System of trade#63835475,Government Accounting concept#63835476,IMF data dissemination standard#63835477,Latest population census#63835478,... 7 more fields] csv\n                  +- Project [country_name#63835517, Population, total#11768599 AS population_total#63837360, Population growth (annual %)#11768594 AS population_growth#63837361, Urban population growth (annual %)#11768712 AS urban_population_growth#63837362, Urban population#11768710 AS urban_population#63837363, Rural population#11768668 AS rural_population#63837364, Unemployment, total (% of total labor force)#11768708 AS unemployment_rate#63837365, age_dependency_ratio_workingage#63836437, Poverty headcount ratio at national poverty line (% of population)#11768601 AS poverty_headcount_percentage#63837366, labor_force_total#63836898, Net migration#11768451 AS net_migration#63837367, year#11767306]\n                     +- Project [Year#11767306, AIDS estimated deaths (UNAIDS estimates)#11768264, ARI treatment (% of children under 5 taken to a health provider)#11768265, Adolescent fertility rate (births per 1,000 women ages 15-19)#11768266, Adults (ages 15+) and children (0-14 years) living with HIV#11768267, Adults (ages 15+) and children (ages 0-14) newly infected with HIV#11768268, Adults (ages 15+) living with HIV#11768269, Adults (ages 15-49) newly infected with HIV#11768270, Age at first marriage, female#11768271, Age at first marriage, male#11768272, Age dependency ratio (% of working-age population)#11768273, Age dependency ratio, old#11768274, Age dependency ratio, young#11768275, Age population, age 00, female, interpolated#11768276, Age population, age 00, male, interpolated#11768277, Age population, age 01, female, interpolated#11768278, Age population, age 01, male, interpolated#11768279, Age population, age 02, female, interpolated#11768280, Age population, age 02, male, interpolated#11768281, Age population, age 03, female, interpolated#11768282, Age population, age 03, male, interpolated#11768283, Age population, age 04, female, interpolated#11768284, Age population, age 04, male, interpolated#11768285, Age population, age 05, female, interpolated#11768286, ... 437 more fields]\n                        +- Project [Year#11767306, AIDS estimated deaths (UNAIDS estimates)#11768264, ARI treatment (% of children under 5 taken to a health provider)#11768265, Adolescent fertility rate (births per 1,000 women ages 15-19)#11768266, Adults (ages 15+) and children (0-14 years) living with HIV#11768267, Adults (ages 15+) and children (ages 0-14) newly infected with HIV#11768268, Adults (ages 15+) living with HIV#11768269, Adults (ages 15-49) newly infected with HIV#11768270, Age at first marriage, female#11768271, Age at first marriage, male#11768272, Age dependency ratio (% of working-age population)#11768273, Age dependency ratio, old#11768274, Age dependency ratio, young#11768275, Age population, age 00, female, interpolated#11768276, Age population, age 00, male, interpolated#11768277, Age population, age 01, female, interpolated#11768278, Age population, age 01, male, interpolated#11768279, Age population, age 02, female, interpolated#11768280, Age population, age 02, male, interpolated#11768281, Age population, age 03, female, interpolated#11768282, Age population, age 03, male, interpolated#11768283, Age population, age 04, female, interpolated#11768284, Age population, age 04, male, interpolated#11768285, Age population, age 05, female, interpolated#11768286, ... 436 more fields]\n                           +- Project [Year#11767306, AIDS estimated deaths (UNAIDS estimates)#11768264, ARI treatment (% of children under 5 taken to a health provider)#11768265, Adolescent fertility rate (births per 1,000 women ages 15-19)#11768266, Adults (ages 15+) and children (0-14 years) living with HIV#11768267, Adults (ages 15+) and children (ages 0-14) newly infected with HIV#11768268, Adults (ages 15+) living with HIV#11768269, Adults (ages 15-49) newly infected with HIV#11768270, Age at first marriage, female#11768271, Age at first marriage, male#11768272, Age dependency ratio (% of working-age population)#11768273, Age dependency ratio, old#11768274, Age dependency ratio, young#11768275, Age population, age 00, female, interpolated#11768276, Age population, age 00, male, interpolated#11768277, Age population, age 01, female, interpolated#11768278, Age population, age 01, male, interpolated#11768279, Age population, age 02, female, interpolated#11768280, Age population, age 02, male, interpolated#11768281, Age population, age 03, female, interpolated#11768282, Age population, age 03, male, interpolated#11768283, Age population, age 04, female, interpolated#11768284, Age population, age 04, male, interpolated#11768285, Age population, age 05, female, interpolated#11768286, ... 435 more fields]\n                              +- Project [Country Name#11767071, Year#11767306, AIDS estimated deaths (UNAIDS estimates)#11768264, ARI treatment (% of children under 5 taken to a health provider)#11768265, Adolescent fertility rate (births per 1,000 women ages 15-19)#11768266, Adults (ages 15+) and children (0-14 years) living with HIV#11768267, Adults (ages 15+) and children (ages 0-14) newly infected with HIV#11768268, Adults (ages 15+) living with HIV#11768269, Adults (ages 15-49) newly infected with HIV#11768270, Age at first marriage, female#11768271, Age at first marriage, male#11768272, Age dependency ratio (% of working-age population)#11768273, Age dependency ratio, old#11768274, Age dependency ratio, young#11768275, Age population, age 00, female, interpolated#11768276, Age population, age 00, male, interpolated#11768277, Age population, age 01, female, interpolated#11768278, Age population, age 01, male, interpolated#11768279, Age population, age 02, female, interpolated#11768280, Age population, age 02, male, interpolated#11768281, Age population, age 03, female, interpolated#11768282, Age population, age 03, male, interpolated#11768283, Age population, age 04, female, interpolated#11768284, Age population, age 04, male, interpolated#11768285, ... 436 more fields]\n                                 +- Project [Country Name#11767071, Year#11767306, __pivot_sum(`Value`) AS `sum(``Value``)`#11768263[0] AS AIDS estimated deaths (UNAIDS estimates)#11768264, __pivot_sum(`Value`) AS `sum(``Value``)`#11768263[1] AS ARI treatment (% of children under 5 taken to a health provider)#11768265, __pivot_sum(`Value`) AS `sum(``Value``)`#11768263[2] AS Adolescent fertility rate (births per 1,000 women ages 15-19)#11768266, __pivot_sum(`Value`) AS `sum(``Value``)`#11768263[3] AS Adults (ages 15+) and children (0-14 years) living with HIV#11768267, __pivot_sum(`Value`) AS `sum(``Value``)`#11768263[4] AS Adults (ages 15+) and children (ages 0-14) newly infected with HIV#11768268, __pivot_sum(`Value`) AS `sum(``Value``)`#11768263[5] AS Adults (ages 15+) living with HIV#11768269, __pivot_sum(`Value`) AS `sum(``Value``)`#11768263[6] AS Adults (ages 15-49) newly infected with HIV#11768270, __pivot_sum(`Value`) AS `sum(``Value``)`#11768263[7] AS Age at first marriage, female#11768271, __pivot_sum(`Value`) AS `sum(``Value``)`#11768263[8] AS Age at first marriage, male#11768272, __pivot_sum(`Value`) AS `sum(``Value``)`#11768263[9] AS Age dependency ratio (% of working-age population)#11768273, __pivot_sum(`Value`) AS `sum(``Value``)`#11768263[10] AS Age dependency ratio, old#11768274, __pivot_sum(`Value`) AS `sum(``Value``)`#11768263[11] AS Age dependency ratio, young#11768275, __pivot_sum(`Value`) AS `sum(``Value``)`#11768263[12] AS Age population, age 00, female, interpolated#11768276, __pivot_sum(`Value`) AS `sum(``Value``)`#11768263[13] AS Age population, age 00, male, interpolated#11768277, __pivot_sum(`Value`) AS `sum(``Value``)`#11768263[14] AS Age population, age 01, female, interpolated#11768278, __pivot_sum(`Value`) AS `sum(``Value``)`#11768263[15] AS Age population, age 01, male, interpolated#11768279, __pivot_sum(`Value`) AS `sum(``Value``)`#11768263[16] AS Age population, age 02, female, interpolated#11768280, __pivot_sum(`Value`) AS `sum(``Value``)`#11768263[17] AS Age population, age 02, male, interpolated#11768281, __pivot_sum(`Value`) AS `sum(``Value``)`#11768263[18] AS Age population, age 03, female, interpolated#11768282, __pivot_sum(`Value`) AS `sum(``Value``)`#11768263[19] AS Age population, age 03, male, interpolated#11768283, __pivot_sum(`Value`) AS `sum(``Value``)`#11768263[20] AS Age population, age 04, female, interpolated#11768284, __pivot_sum(`Value`) AS `sum(``Value``)`#11768263[21] AS Age population, age 04, male, interpolated#11768285, ... 435 more fields]\n                                    +- Aggregate [Country Name#11767071, Year#11767306], [Country Name#11767071, Year#11767306, pivotfirst(Indicator Name#11767073, sum(`Value`)#11767347, AIDS estimated deaths (UNAIDS estimates), ARI treatment (% of children under 5 taken to a health provider), Adolescent fertility rate (births per 1,000 women ages 15-19), Adults (ages 15+) and children (0-14 years) living with HIV, Adults (ages 15+) and children (ages 0-14) newly infected with HIV, Adults (ages 15+) living with HIV, Adults (ages 15-49) newly infected with HIV, Age at first marriage, female, Age at first marriage, male, Age dependency ratio (% of working-age population), Age dependency ratio, old, Age dependency ratio, young, Age population, age 00, female, interpolated, Age population, age 00, male, interpolated, Age population, age 01, female, interpolated, Age population, age 01, male, interpolated, Age population, age 02, female, interpolated, Age population, age 02, male, interpolated, Age population, age 03, female, interpolated, Age population, age 03, male, interpolated, Age population, age 04, female, interpolated, Age population, age 04, male, interpolated, Age population, age 05, female, interpolated, Age population, age 05, male, interpolated, Age population, age 06, female, interpolated, Age population, age 06, male, interpolated, Age population, age 07, female, interpolated, Age population, age 07, male, interpolated, Age population, age 08, female, interpolated, Age population, age 08, male, interpolated, Age population, age 09, female, interpolated, Age population, age 09, male, interpolated, Age population, age 10, female, interpolated, Age population, age 10, male, interpolated, Age population, age 11, female, interpolated, Age population, age 11, male, interpolated, Age population, age 12, female, interpolated, Age population, age 12, male, interpolated, Age population, age 13, female, interpolated, Age population, age 13, male, interpolated, Age population, age 14, female, interpolated, Age population, age 14, male, interpolated, Age population, age 15, female, interpolated, Age population, age 15, male, interpolated, Age population, age 16, female, interpolated, Age population, age 16, male, interpolated, Age population, age 17, female, interpolated, Age population, age 17, male, interpolated, Age population, age 18, female, interpolated, Age population, age 18, male, interpolated, Age population, age 19, female, interpolated, Age population, age 19, male, interpolated, Age population, age 20, female, interpolated, Age population, age 20, male, interpolated, Age population, age 21, female, interpolated, Age population, age 21, male, interpolated, Age population, age 22, female, interpolated, Age population, age 22, male, interpolated, Age population, age 23, female, interpolated, Age population, age 23, male, interpolated, Age population, age 24, female, interpolated, Age population, age 24, male, interpolated, Age population, age 25, female, interpolated, Age population, age 25, male, interpolated, Antiretroviral therapy coverage (% of people living with HIV), Antiretroviral therapy coverage for PMTCT (% of pregnant women living with HIV), Birth rate, crude (per 1,000 people), Births attended by skilled health staff (% of total), Capital health expenditure (% of GDP), Cause of death, by communicable diseases and maternal, prenatal and nutrition conditions (% of total), Cause of death, by injury (% of total), Cause of death, by non-communicable diseases (% of total), Children (0-14) living with HIV, Children (ages 0-14) newly infected with HIV, Children orphaned by HIV/AIDS, Children with fever receiving antimalarial drugs (% of children under age 5 with fever), Community health workers (per 1,000 people), Completeness of birth registration (%), Completeness of birth registration, female (%), Completeness of birth registration, male (%), Completeness of birth registration, rural (%), Completeness of birth registration, urban (%), Completeness of death registration with cause-of-death information (%), Comprehensive correct knowledge of HIV/AIDS, ages 15-24, female (2 prevent ways and reject 3 misconceptions), Comprehensive correct knowledge of HIV/AIDS, ages 15-24, male (2 prevent ways and reject 3 misconceptions), Comprehensive correct knowledge of HIV/AIDS, ages 15-49, female (2 prevent ways and reject 3 misconceptions), Comprehensive correct knowledge of HIV/AIDS, ages 15-49, male (2 prevent ways and reject 3 misconceptions), Condom use at last high-risk sex, adult female (% ages 15-49), Condom use at last high-risk sex, adult male (% ages 15-49), Condom use, population ages 15-24, female (% of females ages 15-24), Condom use, population ages 15-24, male (% of males ages 15-24), Consumption of iodized salt (% of households), Contraceptive prevalence, any method (% of married women ages 15-49), Contraceptive prevalence, any method (% of sexually active unmarried women ages 15-49), Contraceptive prevalence, any modern method (% of married women ages 15-49), Contraceptive prevalence, any modern method (% of sexually active unmarried women ages 15-49), Current health expenditure (% of GDP), Current health expenditure per capita (current US$), Current health expenditure per capita, PPP (current international $), Death rate, crude (per 1,000 people), Demand for family planning satisfied by any methods (% of married women with demand for family planning), Demand for family planning satisfied by modern methods (% of married women with demand for family planning), Diabetes prevalence (% of population ages 20 to 79), Diarrhea treatment (% of children under 5 receiving oral rehydration and continued feeding), Diarrhea treatment (% of children under 5 who received ORS packet), Domestic general government health expenditure (% of GDP), Domestic general government health expenditure (% of current health expenditure), Domestic general government health expenditure (% of general government expenditure), Domestic general government health expenditure per capita (current US$), Domestic general government health expenditure per capita, PPP (current international $), Domestic private health expenditure (% of current health expenditure), Domestic private health expenditure per capita (current US$), Domestic private health expenditure per capita, PPP  (current international $), Exclusive breastfeeding (% of children under 6 months), External health expenditure (% of current health expenditure), External health expenditure channeled through government (% of external health expenditure), External health expenditure per capita (current US$), External health expenditure per capita, PPP (current international $), Female headed households (% of households with a female head), Fertility rate, total (births per woman), GNI per capita, Atlas method (current US$), Hospital beds (per 1,000 people), Human capital index (HCI) (scale 0-1), Human capital index (HCI), female (scale 0-1), Human capital index (HCI), female, lower bound (scale 0-1), Human capital index (HCI), female, upper bound (scale 0-1), Human capital index (HCI), lower bound (scale 0-1), Human capital index (HCI), male (scale 0-1), Human capital index (HCI), male, lower bound (scale 0-1), Human capital index (HCI), male, upper bound (scale 0-1), Human capital index (HCI), upper bound (scale 0-1), Immunization, BCG (% of one-year-old children), Immunization, DPT (% of children ages 12-23 months), Immunization, HepB3 (% of one-year-old children), Immunization, Hib3 (% of children ages 12-23 months), Immunization, Pol3 (% of one-year-old children), Immunization, measles (% of children ages 12-23 months), Immunization, measles second dose (% of children by the nationally recommended age), Incidence of HIV, ages 15-24 (per 1,000 uninfected population ages 15-24), Incidence of HIV, ages 15-49 (per 1,000 uninfected population ages 15-49), Incidence of HIV, ages 50+ (per 1,000 uninfected population ages 50+), Incidence of HIV, all (per 1,000 uninfected population), Incidence of malaria (per 1,000 population at risk), Incidence of tuberculosis (per 100,000 people), Infant and young child feeding practices, all 3 IYCF (% children ages 6-23 months), Intermittent preventive treatment (IPT) of malaria in pregnancy (% of pregnant women), Labor force, female (% of total labor force), Labor force, total, Life expectancy at birth, female (years), Life expectancy at birth, male (years), Life expectancy at birth, total (years), Lifetime risk of maternal death (%), Lifetime risk of maternal death (1 in: rate varies by country), Literacy rate, adult female (% of females ages 15 and above), Literacy rate, adult male (% of males ages 15 and above), Literacy rate, adult total (% of people ages 15 and above), Literacy rate, youth male (% of males ages 15-24), Literacy rate, youth total (% of people ages 15-24), Low-birthweight babies (% of births), Malaria cases reported, Maternal leave benefits (% of wages paid in covered period), Maternal mortality ratio (modeled estimate, per 100,000 live births), Maternal mortality ratio (national estimate, per 100,000 live births), Mortality caused by road traffic injury (per 100,000 people), Mortality caused by road traffic injury, female (per 100,000 female population), Mortality caused by road traffic injury, male (per 100,000 male population), Mortality from CVD, cancer, diabetes or CRD between exact ages 30 and 70 (%), Mortality from CVD, cancer, diabetes or CRD between exact ages 30 and 70, female (%), Mortality from CVD, cancer, diabetes or CRD between exact ages 30 and 70, male (%), Mortality rate attributed to household and ambient air pollution (per 100,000 population), Mortality rate attributed to household and ambient air pollution, age-standardized, female (per 100,000 female population), Mortality rate attributed to household and ambient air pollution, age-standardized, male (per 100,000 male population), Mortality rate attributed to unintentional poisoning (per 100,000 population), Mortality rate attributed to unintentional poisoning, female (per 100,000 female population), Mortality rate attributed to unintentional poisoning, male (per 100,000 male population), Mortality rate attributed to unsafe water, unsafe sanitation and lack of hygiene (per 100,000 population), Mortality rate attributed to unsafe water, unsafe sanitation and lack of hygiene, female (per 100,000 female population), Mortality rate attributed to unsafe water, unsafe sanitation and lack of hygiene, male (per 100,000 male population), Mortality rate, adult, female (per 1,000 female adults), Mortality rate, adult, male (per 1,000 male adults), Mortality rate, infant (per 1,000 live births), Mortality rate, infant, female (per 1,000 live births), Mortality rate, infant, male (per 1,000 live births), Mortality rate, neonatal (per 1,000 live births), Mortality rate, under-5 (per 1,000), Mortality rate, under-5, female (per 1,000), Mortality rate, under-5, male (per 1,000), Net migration, Newborns protected against tetanus (%), Number of deaths ages 10-14 years, Number of deaths ages 10-14 years, female, Number of deaths ages 10-14 years, male, Number of deaths ages 10-19 years, Number of deaths ages 10-19 years, female, Number of deaths ages 10-19 years, male, Number of deaths ages 15-19 years, Number of deaths ages 15-19 years, female, Number of deaths ages 15-19 years, male, Number of deaths ages 20-24 years, Number of deaths ages 20-24 years, female, Number of deaths ages 20-24 years, male, Number of deaths ages 5-9 years, Number of deaths ages 5-9 years, female, Number of deaths ages 5-9 years, male, Number of infant deaths, Number of infant deaths, female, Number of infant deaths, male, Number of maternal deaths, Number of neonatal deaths, Number of people pushed below the $1.90 ($ 2011 PPP) poverty line by out-of-pocket health care expenditure, Number of people pushed below the $3.20 ($ 2011 PPP) poverty line by out-of-pocket health care expenditure, Number of people pushed further below the $1.90 ($ 2011 PPP) poverty line by out-of-pocket health care expenditure, Number of people pushed further below the $3.20 ($ 2011 PPP) poverty line by out-of-pocket health care expenditure, Number of people spending more than 10% of household consumption or income on out-of-pocket health care expenditure, Number of people spending more than 25% of household consumption or income on out-of-pocket health care expenditure, Number of people who are undernourished, Number of stillbirths, Number of surgical procedures (per 100,000 population), Number of under-five deaths, Number of under-five deaths, female, Number of under-five deaths, male, Nurses and midwives (per 1,000 people), Out-of-pocket expenditure (% of current health expenditure), Out-of-pocket expenditure per capita (current US$), Out-of-pocket expenditure per capita, PPP (current international $), People practicing open defecation (% of population), People practicing open defecation, rural (% of rural population), People practicing open defecation, urban (% of urban population), People using at least basic drinking water services (% of population), People using at least basic drinking water services, rural (% of rural population), People using at least basic drinking water services, urban (% of urban population), People using at least basic sanitation services (% of population), People using at least basic sanitation services, rural (% of rural population), People using at least basic sanitation services, urban  (% of urban population), People using safely managed drinking water services (% of population), People using safely managed drinking water services, rural (% of rural population), People using safely managed drinking water services, urban (% of urban population), People using safely managed sanitation services (% of population), People using safely managed sanitation services, rural (% of rural population), People using safely managed sanitation services, urban  (% of urban population), People with basic handwashing facilities including soap and water (% of population), People with basic handwashing facilities including soap and water, rural (% of rural population), People with basic handwashing facilities including soap and water, urban (% of urban population), Physicians (per 1,000 people), Population ages 0-14 (% of total population), Population ages 0-14, female, Population ages 0-14, female (% of female population), Population ages 0-14, male, Population ages 0-14, male (% of male population), Population ages 00-04, female, Population ages 00-04, female (% of female population), Population ages 00-04, male, Population ages 00-04, male (% of male population), Population ages 00-14, total, Population ages 05-09, female, Population ages 05-09, female (% of female population), Population ages 05-09, male, Population ages 05-09, male (% of male population), Population ages 10-14, female, Population ages 10-14, female (% of female population), Population ages 10-14, male, Population ages 10-14, male (% of male population), Population ages 15-19, female, Population ages 15-19, female (% of female population), Population ages 15-19, male, Population ages 15-19, male (% of male population), Population ages 15-64 (% of total population), Population ages 15-64, female, Population ages 15-64, female (% of female population), Population ages 15-64, male, Population ages 15-64, male (% of male population), Population ages 15-64, total, Population ages 20-24, female, Population ages 20-24, female (% of female population), Population ages 20-24, male, Population ages 20-24, male (% of male population), Population ages 25-29, female, Population ages 25-29, female (% of female population), Population ages 25-29, male, Population ages 25-29, male (% of male population), Population ages 30-34, female, Population ages 30-34, female (% of female population), Population ages 30-34, male, Population ages 30-34, male (% of male population), Population ages 35-39, female, Population ages 35-39, female (% of female population), Population ages 35-39, male, Population ages 35-39, male (% of male population), Population ages 40-44, female, Population ages 40-44, female (% of female population), Population ages 40-44, male, Population ages 40-44, male (% of male population), Population ages 45-49, female, Population ages 45-49, female (% of female population), Population ages 45-49, male, Population ages 45-49, male (% of male population), Population ages 50-54, female, Population ages 50-54, female (% of female population), Population ages 50-54, male, Population ages 50-54, male (% of male population), Population ages 55-59, female, Population ages 55-59, female (% of female population), Population ages 55-59, male, Population ages 55-59, male (% of male population), Population ages 60-64, female, Population ages 60-64, female (% of female population), Population ages 60-64, male, Population ages 60-64, male (% of male population), Population ages 65 and above (% of total population), Population ages 65 and above, female, Population ages 65 and above, female (% of female population), Population ages 65 and above, male, Population ages 65 and above, male (% of male population), Population ages 65 and above, total, Population ages 65-69, female, Population ages 65-69, female (% of female population), Population ages 65-69, male, Population ages 65-69, male (% of male population), Population ages 70-74, female, Population ages 70-74, female (% of female population), Population ages 70-74, male, Population ages 70-74, male (% of male population), Population ages 75-79, female, Population ages 75-79, female (% of female population), Population ages 75-79, male, Population ages 75-79, male (% of male population), Population ages 80 and above, female, Population ages 80 and above, male, Population ages 80 and above, male (% of male population), Population ages 80 and older, female (% of female population), Population growth (annual %), Population, female, Population, female (% of total population), Population, male, Population, male (% of total population), Population, total, Postnatal care coverage (% mothers), Poverty headcount ratio at national poverty line (% of population), Pregnant women receiving prenatal care (%), Pregnant women receiving prenatal care of at least four visits (% of pregnant women), Prevalence of HIV, female (% ages 15-24), Prevalence of HIV, male (% ages 15-24), Prevalence of HIV, total (% of population ages 15-49), Prevalence of anemia among children (% of children ages 6-59 months), Prevalence of anemia among non-pregnant women (% of women ages 15-49), Prevalence of anemia among pregnant women (%), Prevalence of anemia among women of reproductive age (% of women ages 15-49), Prevalence of current tobacco use (% of adults), Prevalence of current tobacco use, females (% of female adults), Prevalence of current tobacco use, males (% of male adults), Prevalence of hypertension (% of adults ages 30-79), Prevalence of hypertension, female (% of female adults ages 30-79), Prevalence of hypertension, male (% of male adults ages 30-79), Prevalence of overweight (% of adults), Prevalence of overweight (% of children under 5), Prevalence of overweight (modeled estimate, % of children under 5), Prevalence of overweight, female (% of children under 5), Prevalence of overweight, female (% of female adults), Prevalence of overweight, male (% of children under 5), Prevalence of overweight, male (% of male adults), Prevalence of severe wasting, weight for height (% of children under 5), Prevalence of severe wasting, weight for height, female (% of children under 5), Prevalence of severe wasting, weight for height, male (% of children under 5), Prevalence of stunting, height for age (% of children under 5), Prevalence of stunting, height for age (modeled estimate, % of children under 5), Prevalence of stunting, height for age, female (% of children under 5), Prevalence of stunting, height for age, male (% of children under 5), Prevalence of syphilis (% of women attending antenatal care), Prevalence of undernourishment (% of population), Prevalence of underweight, weight for age (% of children under 5), Prevalence of underweight, weight for age, female (% of children under 5), Prevalence of underweight, weight for age, male (% of children under 5), Prevalence of wasting, weight for height (% of children under 5), Prevalence of wasting, weight for height, female (% of children under 5), Prevalence of wasting, weight for height, male (% of children under 5), Primary completion rate, female (% of relevant age group), Primary completion rate, male (% of relevant age group), Primary completion rate, total (% of relevant age group), Probability of dying among adolescents ages 10-14 years (per 1,000), Probability of dying among adolescents ages 10-14 years, female (per 1,000), Probability of dying among adolescents ages 10-14 years, male (per 1,000), Probability of dying among adolescents ages 10-19 years (per 1,000), Probability of dying among adolescents ages 10-19 years, female (per 1,000), Probability of dying among adolescents ages 10-19 years, male (per 1,000), Probability of dying among adolescents ages 15-19 years (per 1,000), Probability of dying among adolescents ages 15-19 years, female (per 1,000), Probability of dying among adolescents ages 15-19 years, male (per 1,000), Probability of dying among children ages 5-9 years (per 1,000), Probability of dying among children ages 5-9 years, female (per 1,000), Probability of dying among children ages 5-9 years, male (per 1,000), Probability of dying among youth ages 20-24 years (per 1,000), Probability of dying among youth ages 20-24 years, female (per 1,000), Probability of dying among youth ages 20-24 years, male (per 1,000), Proportion of population pushed below the $1.90 ($ 2011 PPP) poverty line by out-of-pocket health care expenditure (%), Proportion of population pushed below the $3.20 ($ 2011 PPP) poverty line by out-of-pocket health care expenditure (%), Proportion of population pushed further below the $1.90 ($ 2011 PPP) poverty line by out-of-pocket health care expenditure (%), Proportion of population pushed further below the $3.20 ($ 2011 PPP) poverty line by out-of-pocket health care expenditure (%), Proportion of population spending more than 10% of household consumption or income on out-of-pocket health care expenditure (%), Proportion of population spending more than 25% of household consumption or income on out-of-pocket health care expenditure (%), Public spending on education, total (% of GDP), Ratio of school attendance of orphans to school attendance of non-orphans ages 10-14, Ratio of young literate females to males (% ages 15-24), Risk of catastrophic expenditure for surgical care (% of people at risk), Risk of impoverishing expenditure for surgical care (% of people at risk), Rural population, Rural population (% of total population), Rural population growth (annual %), Rural poverty headcount ratio at national poverty lines (% of rural population), School enrollment, primary (% gross), School enrollment, primary (% net), School enrollment, primary, female (% gross), School enrollment, primary, female (% net), School enrollment, primary, male (% gross), School enrollment, primary, male (% net), School enrollment, secondary (% gross), School enrollment, secondary (% net), School enrollment, secondary, female (% gross), School enrollment, secondary, female (% net), School enrollment, secondary, male (% gross), School enrollment, secondary, male (% net), School enrollment, tertiary (% gross), School enrollment, tertiary, female (% gross), Sex ratio at birth (male births per female births), Share of women employed in the nonagricultural sector (% of total nonagricultural employment), Specialist surgical workforce (per 100,000 population), Stillbirth rate (per 1,000 total births), Suicide mortality rate (per 100,000 population), Suicide mortality rate, female (per 100,000 female population), Suicide mortality rate, male (per 100,000 male population), Survival to age 65, female (% of cohort), Survival to age 65, male (% of cohort), Teenage mothers (% of women ages 15-19 who have had children or are currently pregnant), Total alcohol consumption per capita (liters of pure alcohol, projected estimates, 15+ years of age), Total alcohol consumption per capita, female (liters of pure alcohol, projected estimates, female 15+ years of age), Total alcohol consumption per capita, male (liters of pure alcohol, projected estimates, male 15+ years of age), Treatment for hypertension (% of adults ages 30-79 with hypertension), Treatment for hypertension, female (% of female adults ages 30-79 with hypertension), Treatment for hypertension, male (% of male adults ages 30-79 with hypertension), Tuberculosis case detection rate (%, all forms), Tuberculosis death rate (per 100,000 people), Tuberculosis treatment success rate (% of new cases), UHC service coverage index, Unemployment, female (% of female labor force), Unemployment, male (% of male labor force), Unemployment, total (% of total labor force), Unmet need for contraception (% of married women ages 15-49), Urban population, Urban population (% of total population), Urban population growth (annual %), Urban poverty headcount ratio at national poverty lines (% of urban population), Use of insecticide-treated bed nets (% of under-5 population), Vitamin A supplementation coverage rate (% of children ages 6-59 months), Wanted fertility rate (births per woman), Women who were first married by age 15 (% of women ages 20-24), Women who were first married by age 18 (% of women ages 20-24), Women's share of population ages 15+ living with HIV (%), Young people (ages 15-24) newly infected with HIV, 0, 0) AS __pivot_sum(`Value`) AS `sum(``Value``)`#11768263]\n                                       +- Aggregate [Country Name#11767071, Year#11767306, Indicator Name#11767073], [Country Name#11767071, Year#11767306, Indicator Name#11767073, sum(Value#11767307) AS sum(`Value`)#11767347]\n                                          +- Project [Country Name#11767071, Country Code#11767072, Indicator Name#11767073, Year#11767306, Value#11767307, month(cast(date#11767281 as date)) AS month#11767303, dayofmonth(cast(date#11767281 as date)) AS day#11767304, quarter(cast(date#11767281 as date)) AS quarter#11767305]\n                                             +- Generate stack(16, 2005, 2005#11767245, 2006, 2006#11767246, 2007, 2007#11767247, 2008, 2008#11767248, 2009, 2009#11767249, 2010, 2010#11767250, 2011, 2011#11767251, 2012, 2012#11767252, 2013, 2013#11767253, 2014, 2014#11767254, 2015, 2015#11767255, 2016, ... 9 more fields), false, [Year#11767306, Value#11767307]\n                                                +- Project [Country Name#11767071, Country Code#11767072, Indicator Name#11767073, Indicator Code#11767074, 2005#11767245, 2006#11767246, 2007#11767247, 2008#11767248, 2009#11767249, 2010#11767250, 2011#11767251, 2012#11767252, 2013#11767253, 2014#11767254, 2015#11767255, 2016#11767256, 2017#11767257, 2018#11767258, 2019#11767259, 2020#11767260, date_format(cast(2020-04-01 as timestamp), yyyy-MM-dd, Some(America/Toronto)) AS date#11767281]\n                                                   +- Project [Country Name#11767071, Country Code#11767072, Indicator Name#11767073, Indicator Code#11767074, coalesce(nanvl(2005#11767120, cast(null as double)), cast(0.0 as double)) AS 2005#11767245, coalesce(nanvl(2006#11767121, cast(null as double)), cast(0.0 as double)) AS 2006#11767246, coalesce(nanvl(2007#11767122, cast(null as double)), cast(0.0 as double)) AS 2007#11767247, coalesce(nanvl(2008#11767123, cast(null as double)), cast(0.0 as double)) AS 2008#11767248, coalesce(nanvl(2009#11767124, cast(null as double)), cast(0.0 as double)) AS 2009#11767249, coalesce(nanvl(2010#11767125, cast(null as double)), cast(0.0 as double)) AS 2010#11767250, coalesce(nanvl(2011#11767126, cast(null as double)), cast(0.0 as double)) AS 2011#11767251, coalesce(nanvl(2012#11767127, cast(null as double)), cast(0.0 as double)) AS 2012#11767252, coalesce(nanvl(2013#11767128, cast(null as double)), cast(0.0 as double)) AS 2013#11767253, coalesce(nanvl(2014#11767129, cast(null as double)), cast(0.0 as double)) AS 2014#11767254, coalesce(nanvl(2015#11767130, cast(null as double)), cast(0.0 as double)) AS 2015#11767255, coalesce(nanvl(2016#11767131, cast(null as double)), cast(0.0 as double)) AS 2016#11767256, coalesce(nanvl(2017#11767132, cast(null as double)), cast(0.0 as double)) AS 2017#11767257, coalesce(nanvl(2018#11767133, cast(null as double)), cast(0.0 as double)) AS 2018#11767258, coalesce(nanvl(2019#11767134, cast(null as double)), cast(0.0 as double)) AS 2019#11767259, coalesce(nanvl(2020#11767135, cast(null as double)), cast(0.0 as double)) AS 2020#11767260]\n                                                      +- Filter Country Name#11767071 IN (United States,Canada,Mexico,Thailand,Finland,Nigeria,Somalia,Norway,Japan)\n                                                         +- Project [Country Name#11767071, Country Code#11767072, Indicator Name#11767073, Indicator Code#11767074, 2005#11767120, 2006#11767121, 2007#11767122, 2008#11767123, 2009#11767124, 2010#11767125, 2011#11767126, 2012#11767127, 2013#11767128, 2014#11767129, 2015#11767130, 2016#11767131, 2017#11767132, 2018#11767133, 2019#11767134, 2020#11767135]\n                                                            +- Relation[Country Name#11767071,Country Code#11767072,Indicator Name#11767073,Indicator Code#11767074,1960#11767075,1961#11767076,1962#11767077,1963#11767078,1964#11767079,1965#11767080,1966#11767081,1967#11767082,1968#11767083,1969#11767084,1970#11767085,1971#11767086,1972#11767087,1973#11767088,1974#11767089,1975#11767090,1976#11767091,1977#11767092,1978#11767093,1979#11767094,... 43 more fields] csv\n"
     ]
    }
   ],
   "source": [
    "#\"Total Damages ('000 US$)\"\n",
    "summary_df(tmp,\"net_migration\",bns=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0e6b304e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Country Name',\n",
       " 'Year',\n",
       " 'AIDS estimated deaths (UNAIDS estimates)',\n",
       " 'ARI treatment (% of children under 5 taken to a health provider)',\n",
       " 'Adolescent fertility rate (births per 1,000 women ages 15-19)',\n",
       " 'Adults (ages 15+) and children (0-14 years) living with HIV',\n",
       " 'Adults (ages 15+) and children (ages 0-14) newly infected with HIV',\n",
       " 'Adults (ages 15+) living with HIV',\n",
       " 'Adults (ages 15-49) newly infected with HIV',\n",
       " 'Age at first marriage, female',\n",
       " 'Age at first marriage, male',\n",
       " 'Age dependency ratio (% of working-age population)',\n",
       " 'Age dependency ratio, old',\n",
       " 'Age dependency ratio, young',\n",
       " 'Age population, age 00, female, interpolated',\n",
       " 'Age population, age 00, male, interpolated',\n",
       " 'Age population, age 01, female, interpolated',\n",
       " 'Age population, age 01, male, interpolated',\n",
       " 'Age population, age 02, female, interpolated',\n",
       " 'Age population, age 02, male, interpolated',\n",
       " 'Age population, age 03, female, interpolated',\n",
       " 'Age population, age 03, male, interpolated',\n",
       " 'Age population, age 04, female, interpolated',\n",
       " 'Age population, age 04, male, interpolated',\n",
       " 'Age population, age 05, female, interpolated',\n",
       " 'Age population, age 05, male, interpolated',\n",
       " 'Age population, age 06, female, interpolated',\n",
       " 'Age population, age 06, male, interpolated',\n",
       " 'Age population, age 07, female, interpolated',\n",
       " 'Age population, age 07, male, interpolated',\n",
       " 'Age population, age 08, female, interpolated',\n",
       " 'Age population, age 08, male, interpolated',\n",
       " 'Age population, age 09, female, interpolated',\n",
       " 'Age population, age 09, male, interpolated',\n",
       " 'Age population, age 10, female, interpolated',\n",
       " 'Age population, age 10, male, interpolated',\n",
       " 'Age population, age 11, female, interpolated',\n",
       " 'Age population, age 11, male, interpolated',\n",
       " 'Age population, age 12, female, interpolated',\n",
       " 'Age population, age 12, male, interpolated',\n",
       " 'Age population, age 13, female, interpolated',\n",
       " 'Age population, age 13, male, interpolated',\n",
       " 'Age population, age 14, female, interpolated',\n",
       " 'Age population, age 14, male, interpolated',\n",
       " 'Age population, age 15, female, interpolated',\n",
       " 'Age population, age 15, male, interpolated',\n",
       " 'Age population, age 16, female, interpolated',\n",
       " 'Age population, age 16, male, interpolated',\n",
       " 'Age population, age 17, female, interpolated',\n",
       " 'Age population, age 17, male, interpolated',\n",
       " 'Age population, age 18, female, interpolated',\n",
       " 'Age population, age 18, male, interpolated',\n",
       " 'Age population, age 19, female, interpolated',\n",
       " 'Age population, age 19, male, interpolated',\n",
       " 'Age population, age 20, female, interpolated',\n",
       " 'Age population, age 20, male, interpolated',\n",
       " 'Age population, age 21, female, interpolated',\n",
       " 'Age population, age 21, male, interpolated',\n",
       " 'Age population, age 22, female, interpolated',\n",
       " 'Age population, age 22, male, interpolated',\n",
       " 'Age population, age 23, female, interpolated',\n",
       " 'Age population, age 23, male, interpolated',\n",
       " 'Age population, age 24, female, interpolated',\n",
       " 'Age population, age 24, male, interpolated',\n",
       " 'Age population, age 25, female, interpolated',\n",
       " 'Age population, age 25, male, interpolated',\n",
       " 'Antiretroviral therapy coverage (% of people living with HIV)',\n",
       " 'Antiretroviral therapy coverage for PMTCT (% of pregnant women living with HIV)',\n",
       " 'Birth rate, crude (per 1,000 people)',\n",
       " 'Births attended by skilled health staff (% of total)',\n",
       " 'Capital health expenditure (% of GDP)',\n",
       " 'Cause of death, by communicable diseases and maternal, prenatal and nutrition conditions (% of total)',\n",
       " 'Cause of death, by injury (% of total)',\n",
       " 'Cause of death, by non-communicable diseases (% of total)',\n",
       " 'Children (0-14) living with HIV',\n",
       " 'Children (ages 0-14) newly infected with HIV',\n",
       " 'Children orphaned by HIV/AIDS',\n",
       " 'Children with fever receiving antimalarial drugs (% of children under age 5 with fever)',\n",
       " 'Community health workers (per 1,000 people)',\n",
       " 'Completeness of birth registration (%)',\n",
       " 'Completeness of birth registration, female (%)',\n",
       " 'Completeness of birth registration, male (%)',\n",
       " 'Completeness of birth registration, rural (%)',\n",
       " 'Completeness of birth registration, urban (%)',\n",
       " 'Completeness of death registration with cause-of-death information (%)',\n",
       " 'Comprehensive correct knowledge of HIV/AIDS, ages 15-24, female (2 prevent ways and reject 3 misconceptions)',\n",
       " 'Comprehensive correct knowledge of HIV/AIDS, ages 15-24, male (2 prevent ways and reject 3 misconceptions)',\n",
       " 'Comprehensive correct knowledge of HIV/AIDS, ages 15-49, female (2 prevent ways and reject 3 misconceptions)',\n",
       " 'Comprehensive correct knowledge of HIV/AIDS, ages 15-49, male (2 prevent ways and reject 3 misconceptions)',\n",
       " 'Condom use at last high-risk sex, adult female (% ages 15-49)',\n",
       " 'Condom use at last high-risk sex, adult male (% ages 15-49)',\n",
       " 'Condom use, population ages 15-24, female (% of females ages 15-24)',\n",
       " 'Condom use, population ages 15-24, male (% of males ages 15-24)',\n",
       " 'Consumption of iodized salt (% of households)',\n",
       " 'Contraceptive prevalence, any method (% of married women ages 15-49)',\n",
       " 'Contraceptive prevalence, any method (% of sexually active unmarried women ages 15-49)',\n",
       " 'Contraceptive prevalence, any modern method (% of married women ages 15-49)',\n",
       " 'Contraceptive prevalence, any modern method (% of sexually active unmarried women ages 15-49)',\n",
       " 'Current health expenditure (% of GDP)',\n",
       " 'Current health expenditure per capita (current US$)',\n",
       " 'Current health expenditure per capita, PPP (current international $)',\n",
       " 'Death rate, crude (per 1,000 people)',\n",
       " 'Demand for family planning satisfied by any methods (% of married women with demand for family planning)',\n",
       " 'Demand for family planning satisfied by modern methods (% of married women with demand for family planning)',\n",
       " 'Diabetes prevalence (% of population ages 20 to 79)',\n",
       " 'Diarrhea treatment (% of children under 5 receiving oral rehydration and continued feeding)',\n",
       " 'Diarrhea treatment (% of children under 5 who received ORS packet)',\n",
       " 'Domestic general government health expenditure (% of GDP)',\n",
       " 'Domestic general government health expenditure (% of current health expenditure)',\n",
       " 'Domestic general government health expenditure (% of general government expenditure)',\n",
       " 'Domestic general government health expenditure per capita (current US$)',\n",
       " 'Domestic general government health expenditure per capita, PPP (current international $)',\n",
       " 'Domestic private health expenditure (% of current health expenditure)',\n",
       " 'Domestic private health expenditure per capita (current US$)',\n",
       " 'Domestic private health expenditure per capita, PPP  (current international $)',\n",
       " 'Exclusive breastfeeding (% of children under 6 months)',\n",
       " 'External health expenditure (% of current health expenditure)',\n",
       " 'External health expenditure channeled through government (% of external health expenditure)',\n",
       " 'External health expenditure per capita (current US$)',\n",
       " 'External health expenditure per capita, PPP (current international $)',\n",
       " 'Female headed households (% of households with a female head)',\n",
       " 'Fertility rate, total (births per woman)',\n",
       " 'GNI per capita, Atlas method (current US$)',\n",
       " 'Hospital beds (per 1,000 people)',\n",
       " 'Human capital index (HCI) (scale 0-1)',\n",
       " 'Human capital index (HCI), female (scale 0-1)',\n",
       " 'Human capital index (HCI), female, lower bound (scale 0-1)',\n",
       " 'Human capital index (HCI), female, upper bound (scale 0-1)',\n",
       " 'Human capital index (HCI), lower bound (scale 0-1)',\n",
       " 'Human capital index (HCI), male (scale 0-1)',\n",
       " 'Human capital index (HCI), male, lower bound (scale 0-1)',\n",
       " 'Human capital index (HCI), male, upper bound (scale 0-1)',\n",
       " 'Human capital index (HCI), upper bound (scale 0-1)',\n",
       " 'Immunization, BCG (% of one-year-old children)',\n",
       " 'Immunization, DPT (% of children ages 12-23 months)',\n",
       " 'Immunization, HepB3 (% of one-year-old children)',\n",
       " 'Immunization, Hib3 (% of children ages 12-23 months)',\n",
       " 'Immunization, Pol3 (% of one-year-old children)',\n",
       " 'Immunization, measles (% of children ages 12-23 months)',\n",
       " 'Immunization, measles second dose (% of children by the nationally recommended age)',\n",
       " 'Incidence of HIV, ages 15-24 (per 1,000 uninfected population ages 15-24)',\n",
       " 'Incidence of HIV, ages 15-49 (per 1,000 uninfected population ages 15-49)',\n",
       " 'Incidence of HIV, ages 50+ (per 1,000 uninfected population ages 50+)',\n",
       " 'Incidence of HIV, all (per 1,000 uninfected population)',\n",
       " 'Incidence of malaria (per 1,000 population at risk)',\n",
       " 'Incidence of tuberculosis (per 100,000 people)',\n",
       " 'Infant and young child feeding practices, all 3 IYCF (% children ages 6-23 months)',\n",
       " 'Intermittent preventive treatment (IPT) of malaria in pregnancy (% of pregnant women)',\n",
       " 'Labor force, female (% of total labor force)',\n",
       " 'Labor force, total',\n",
       " 'Life expectancy at birth, female (years)',\n",
       " 'Life expectancy at birth, male (years)',\n",
       " 'Life expectancy at birth, total (years)',\n",
       " 'Lifetime risk of maternal death (%)',\n",
       " 'Lifetime risk of maternal death (1 in: rate varies by country)',\n",
       " 'Literacy rate, adult female (% of females ages 15 and above)',\n",
       " 'Literacy rate, adult male (% of males ages 15 and above)',\n",
       " 'Literacy rate, adult total (% of people ages 15 and above)',\n",
       " 'Literacy rate, youth male (% of males ages 15-24)',\n",
       " 'Literacy rate, youth total (% of people ages 15-24)',\n",
       " 'Low-birthweight babies (% of births)',\n",
       " 'Malaria cases reported',\n",
       " 'Maternal leave benefits (% of wages paid in covered period)',\n",
       " 'Maternal mortality ratio (modeled estimate, per 100,000 live births)',\n",
       " 'Maternal mortality ratio (national estimate, per 100,000 live births)',\n",
       " 'Mortality caused by road traffic injury (per 100,000 people)',\n",
       " 'Mortality caused by road traffic injury, female (per 100,000 female population)',\n",
       " 'Mortality caused by road traffic injury, male (per 100,000 male population)',\n",
       " 'Mortality from CVD, cancer, diabetes or CRD between exact ages 30 and 70 (%)',\n",
       " 'Mortality from CVD, cancer, diabetes or CRD between exact ages 30 and 70, female (%)',\n",
       " 'Mortality from CVD, cancer, diabetes or CRD between exact ages 30 and 70, male (%)',\n",
       " 'Mortality rate attributed to household and ambient air pollution (per 100,000 population)',\n",
       " 'Mortality rate attributed to household and ambient air pollution, age-standardized, female (per 100,000 female population)',\n",
       " 'Mortality rate attributed to household and ambient air pollution, age-standardized, male (per 100,000 male population)',\n",
       " 'Mortality rate attributed to unintentional poisoning (per 100,000 population)',\n",
       " 'Mortality rate attributed to unintentional poisoning, female (per 100,000 female population)',\n",
       " 'Mortality rate attributed to unintentional poisoning, male (per 100,000 male population)',\n",
       " 'Mortality rate attributed to unsafe water, unsafe sanitation and lack of hygiene (per 100,000 population)',\n",
       " 'Mortality rate attributed to unsafe water, unsafe sanitation and lack of hygiene, female (per 100,000 female population)',\n",
       " 'Mortality rate attributed to unsafe water, unsafe sanitation and lack of hygiene, male (per 100,000 male population)',\n",
       " 'Mortality rate, adult, female (per 1,000 female adults)',\n",
       " 'Mortality rate, adult, male (per 1,000 male adults)',\n",
       " 'Mortality rate, infant (per 1,000 live births)',\n",
       " 'Mortality rate, infant, female (per 1,000 live births)',\n",
       " 'Mortality rate, infant, male (per 1,000 live births)',\n",
       " 'Mortality rate, neonatal (per 1,000 live births)',\n",
       " 'Mortality rate, under-5 (per 1,000)',\n",
       " 'Mortality rate, under-5, female (per 1,000)',\n",
       " 'Mortality rate, under-5, male (per 1,000)',\n",
       " 'Net migration',\n",
       " 'Newborns protected against tetanus (%)',\n",
       " 'Number of deaths ages 10-14 years',\n",
       " 'Number of deaths ages 10-14 years, female',\n",
       " 'Number of deaths ages 10-14 years, male',\n",
       " 'Number of deaths ages 10-19 years',\n",
       " 'Number of deaths ages 10-19 years, female',\n",
       " 'Number of deaths ages 10-19 years, male',\n",
       " 'Number of deaths ages 15-19 years',\n",
       " 'Number of deaths ages 15-19 years, female',\n",
       " 'Number of deaths ages 15-19 years, male',\n",
       " 'Number of deaths ages 20-24 years',\n",
       " 'Number of deaths ages 20-24 years, female',\n",
       " 'Number of deaths ages 20-24 years, male',\n",
       " 'Number of deaths ages 5-9 years',\n",
       " 'Number of deaths ages 5-9 years, female',\n",
       " 'Number of deaths ages 5-9 years, male',\n",
       " 'Number of infant deaths',\n",
       " 'Number of infant deaths, female',\n",
       " 'Number of infant deaths, male',\n",
       " 'Number of maternal deaths',\n",
       " 'Number of neonatal deaths',\n",
       " 'Number of people pushed below the $1.90 ($ 2011 PPP) poverty line by out-of-pocket health care expenditure',\n",
       " 'Number of people pushed below the $3.20 ($ 2011 PPP) poverty line by out-of-pocket health care expenditure',\n",
       " 'Number of people pushed further below the $1.90 ($ 2011 PPP) poverty line by out-of-pocket health care expenditure',\n",
       " 'Number of people pushed further below the $3.20 ($ 2011 PPP) poverty line by out-of-pocket health care expenditure',\n",
       " 'Number of people spending more than 10% of household consumption or income on out-of-pocket health care expenditure',\n",
       " 'Number of people spending more than 25% of household consumption or income on out-of-pocket health care expenditure',\n",
       " 'Number of people who are undernourished',\n",
       " 'Number of stillbirths',\n",
       " 'Number of surgical procedures (per 100,000 population)',\n",
       " 'Number of under-five deaths',\n",
       " 'Number of under-five deaths, female',\n",
       " 'Number of under-five deaths, male',\n",
       " 'Nurses and midwives (per 1,000 people)',\n",
       " 'Out-of-pocket expenditure (% of current health expenditure)',\n",
       " 'Out-of-pocket expenditure per capita (current US$)',\n",
       " 'Out-of-pocket expenditure per capita, PPP (current international $)',\n",
       " 'People practicing open defecation (% of population)',\n",
       " 'People practicing open defecation, rural (% of rural population)',\n",
       " 'People practicing open defecation, urban (% of urban population)',\n",
       " 'People using at least basic drinking water services (% of population)',\n",
       " 'People using at least basic drinking water services, rural (% of rural population)',\n",
       " 'People using at least basic drinking water services, urban (% of urban population)',\n",
       " 'People using at least basic sanitation services (% of population)',\n",
       " 'People using at least basic sanitation services, rural (% of rural population)',\n",
       " 'People using at least basic sanitation services, urban  (% of urban population)',\n",
       " 'People using safely managed drinking water services (% of population)',\n",
       " 'People using safely managed drinking water services, rural (% of rural population)',\n",
       " 'People using safely managed drinking water services, urban (% of urban population)',\n",
       " 'People using safely managed sanitation services (% of population)',\n",
       " 'People using safely managed sanitation services, rural (% of rural population)',\n",
       " 'People using safely managed sanitation services, urban  (% of urban population)',\n",
       " 'People with basic handwashing facilities including soap and water (% of population)',\n",
       " 'People with basic handwashing facilities including soap and water, rural (% of rural population)',\n",
       " 'People with basic handwashing facilities including soap and water, urban (% of urban population)',\n",
       " 'Physicians (per 1,000 people)',\n",
       " 'Population ages 0-14 (% of total population)',\n",
       " 'Population ages 0-14, female',\n",
       " 'Population ages 0-14, female (% of female population)',\n",
       " 'Population ages 0-14, male',\n",
       " 'Population ages 0-14, male (% of male population)',\n",
       " 'Population ages 00-04, female',\n",
       " 'Population ages 00-04, female (% of female population)',\n",
       " 'Population ages 00-04, male',\n",
       " 'Population ages 00-04, male (% of male population)',\n",
       " 'Population ages 00-14, total',\n",
       " 'Population ages 05-09, female',\n",
       " 'Population ages 05-09, female (% of female population)',\n",
       " 'Population ages 05-09, male',\n",
       " 'Population ages 05-09, male (% of male population)',\n",
       " 'Population ages 10-14, female',\n",
       " 'Population ages 10-14, female (% of female population)',\n",
       " 'Population ages 10-14, male',\n",
       " 'Population ages 10-14, male (% of male population)',\n",
       " 'Population ages 15-19, female',\n",
       " 'Population ages 15-19, female (% of female population)',\n",
       " 'Population ages 15-19, male',\n",
       " 'Population ages 15-19, male (% of male population)',\n",
       " 'Population ages 15-64 (% of total population)',\n",
       " 'Population ages 15-64, female',\n",
       " 'Population ages 15-64, female (% of female population)',\n",
       " 'Population ages 15-64, male',\n",
       " 'Population ages 15-64, male (% of male population)',\n",
       " 'Population ages 15-64, total',\n",
       " 'Population ages 20-24, female',\n",
       " 'Population ages 20-24, female (% of female population)',\n",
       " 'Population ages 20-24, male',\n",
       " 'Population ages 20-24, male (% of male population)',\n",
       " 'Population ages 25-29, female',\n",
       " 'Population ages 25-29, female (% of female population)',\n",
       " 'Population ages 25-29, male',\n",
       " 'Population ages 25-29, male (% of male population)',\n",
       " 'Population ages 30-34, female',\n",
       " 'Population ages 30-34, female (% of female population)',\n",
       " 'Population ages 30-34, male',\n",
       " 'Population ages 30-34, male (% of male population)',\n",
       " 'Population ages 35-39, female',\n",
       " 'Population ages 35-39, female (% of female population)',\n",
       " 'Population ages 35-39, male',\n",
       " 'Population ages 35-39, male (% of male population)',\n",
       " 'Population ages 40-44, female',\n",
       " 'Population ages 40-44, female (% of female population)',\n",
       " 'Population ages 40-44, male',\n",
       " 'Population ages 40-44, male (% of male population)',\n",
       " 'Population ages 45-49, female',\n",
       " 'Population ages 45-49, female (% of female population)',\n",
       " 'Population ages 45-49, male',\n",
       " 'Population ages 45-49, male (% of male population)',\n",
       " 'Population ages 50-54, female',\n",
       " 'Population ages 50-54, female (% of female population)',\n",
       " 'Population ages 50-54, male',\n",
       " 'Population ages 50-54, male (% of male population)',\n",
       " 'Population ages 55-59, female',\n",
       " 'Population ages 55-59, female (% of female population)',\n",
       " 'Population ages 55-59, male',\n",
       " 'Population ages 55-59, male (% of male population)',\n",
       " 'Population ages 60-64, female',\n",
       " 'Population ages 60-64, female (% of female population)',\n",
       " 'Population ages 60-64, male',\n",
       " 'Population ages 60-64, male (% of male population)',\n",
       " 'Population ages 65 and above (% of total population)',\n",
       " 'Population ages 65 and above, female',\n",
       " 'Population ages 65 and above, female (% of female population)',\n",
       " 'Population ages 65 and above, male',\n",
       " 'Population ages 65 and above, male (% of male population)',\n",
       " 'Population ages 65 and above, total',\n",
       " 'Population ages 65-69, female',\n",
       " 'Population ages 65-69, female (% of female population)',\n",
       " 'Population ages 65-69, male',\n",
       " 'Population ages 65-69, male (% of male population)',\n",
       " 'Population ages 70-74, female',\n",
       " 'Population ages 70-74, female (% of female population)',\n",
       " 'Population ages 70-74, male',\n",
       " 'Population ages 70-74, male (% of male population)',\n",
       " 'Population ages 75-79, female',\n",
       " 'Population ages 75-79, female (% of female population)',\n",
       " 'Population ages 75-79, male',\n",
       " 'Population ages 75-79, male (% of male population)',\n",
       " 'Population ages 80 and above, female',\n",
       " 'Population ages 80 and above, male',\n",
       " 'Population ages 80 and above, male (% of male population)',\n",
       " 'Population ages 80 and older, female (% of female population)',\n",
       " 'Population growth (annual %)',\n",
       " 'Population, female',\n",
       " 'Population, female (% of total population)',\n",
       " 'Population, male',\n",
       " 'Population, male (% of total population)',\n",
       " 'Population, total',\n",
       " 'Postnatal care coverage (% mothers)',\n",
       " 'Poverty headcount ratio at national poverty line (% of population)',\n",
       " 'Pregnant women receiving prenatal care (%)',\n",
       " 'Pregnant women receiving prenatal care of at least four visits (% of pregnant women)',\n",
       " 'Prevalence of HIV, female (% ages 15-24)',\n",
       " 'Prevalence of HIV, male (% ages 15-24)',\n",
       " 'Prevalence of HIV, total (% of population ages 15-49)',\n",
       " 'Prevalence of anemia among children (% of children ages 6-59 months)',\n",
       " 'Prevalence of anemia among non-pregnant women (% of women ages 15-49)',\n",
       " 'Prevalence of anemia among pregnant women (%)',\n",
       " 'Prevalence of anemia among women of reproductive age (% of women ages 15-49)',\n",
       " 'Prevalence of current tobacco use (% of adults)',\n",
       " 'Prevalence of current tobacco use, females (% of female adults)',\n",
       " 'Prevalence of current tobacco use, males (% of male adults)',\n",
       " 'Prevalence of hypertension (% of adults ages 30-79)',\n",
       " 'Prevalence of hypertension, female (% of female adults ages 30-79)',\n",
       " 'Prevalence of hypertension, male (% of male adults ages 30-79)',\n",
       " 'Prevalence of overweight (% of adults)',\n",
       " 'Prevalence of overweight (% of children under 5)',\n",
       " 'Prevalence of overweight (modeled estimate, % of children under 5)',\n",
       " 'Prevalence of overweight, female (% of children under 5)',\n",
       " 'Prevalence of overweight, female (% of female adults)',\n",
       " 'Prevalence of overweight, male (% of children under 5)',\n",
       " 'Prevalence of overweight, male (% of male adults)',\n",
       " 'Prevalence of severe wasting, weight for height (% of children under 5)',\n",
       " 'Prevalence of severe wasting, weight for height, female (% of children under 5)',\n",
       " 'Prevalence of severe wasting, weight for height, male (% of children under 5)',\n",
       " 'Prevalence of stunting, height for age (% of children under 5)',\n",
       " 'Prevalence of stunting, height for age (modeled estimate, % of children under 5)',\n",
       " 'Prevalence of stunting, height for age, female (% of children under 5)',\n",
       " 'Prevalence of stunting, height for age, male (% of children under 5)',\n",
       " 'Prevalence of syphilis (% of women attending antenatal care)',\n",
       " 'Prevalence of undernourishment (% of population)',\n",
       " 'Prevalence of underweight, weight for age (% of children under 5)',\n",
       " 'Prevalence of underweight, weight for age, female (% of children under 5)',\n",
       " 'Prevalence of underweight, weight for age, male (% of children under 5)',\n",
       " 'Prevalence of wasting, weight for height (% of children under 5)',\n",
       " 'Prevalence of wasting, weight for height, female (% of children under 5)',\n",
       " 'Prevalence of wasting, weight for height, male (% of children under 5)',\n",
       " 'Primary completion rate, female (% of relevant age group)',\n",
       " 'Primary completion rate, male (% of relevant age group)',\n",
       " 'Primary completion rate, total (% of relevant age group)',\n",
       " 'Probability of dying among adolescents ages 10-14 years (per 1,000)',\n",
       " 'Probability of dying among adolescents ages 10-14 years, female (per 1,000)',\n",
       " 'Probability of dying among adolescents ages 10-14 years, male (per 1,000)',\n",
       " 'Probability of dying among adolescents ages 10-19 years (per 1,000)',\n",
       " 'Probability of dying among adolescents ages 10-19 years, female (per 1,000)',\n",
       " 'Probability of dying among adolescents ages 10-19 years, male (per 1,000)',\n",
       " 'Probability of dying among adolescents ages 15-19 years (per 1,000)',\n",
       " 'Probability of dying among adolescents ages 15-19 years, female (per 1,000)',\n",
       " 'Probability of dying among adolescents ages 15-19 years, male (per 1,000)',\n",
       " 'Probability of dying among children ages 5-9 years (per 1,000)',\n",
       " 'Probability of dying among children ages 5-9 years, female (per 1,000)',\n",
       " 'Probability of dying among children ages 5-9 years, male (per 1,000)',\n",
       " 'Probability of dying among youth ages 20-24 years (per 1,000)',\n",
       " 'Probability of dying among youth ages 20-24 years, female (per 1,000)',\n",
       " 'Probability of dying among youth ages 20-24 years, male (per 1,000)',\n",
       " 'Proportion of population pushed below the $1.90 ($ 2011 PPP) poverty line by out-of-pocket health care expenditure (%)',\n",
       " 'Proportion of population pushed below the $3.20 ($ 2011 PPP) poverty line by out-of-pocket health care expenditure (%)',\n",
       " 'Proportion of population pushed further below the $1.90 ($ 2011 PPP) poverty line by out-of-pocket health care expenditure (%)',\n",
       " 'Proportion of population pushed further below the $3.20 ($ 2011 PPP) poverty line by out-of-pocket health care expenditure (%)',\n",
       " 'Proportion of population spending more than 10% of household consumption or income on out-of-pocket health care expenditure (%)',\n",
       " 'Proportion of population spending more than 25% of household consumption or income on out-of-pocket health care expenditure (%)',\n",
       " 'Public spending on education, total (% of GDP)',\n",
       " 'Ratio of school attendance of orphans to school attendance of non-orphans ages 10-14',\n",
       " 'Ratio of young literate females to males (% ages 15-24)',\n",
       " 'Risk of catastrophic expenditure for surgical care (% of people at risk)',\n",
       " 'Risk of impoverishing expenditure for surgical care (% of people at risk)',\n",
       " 'Rural population',\n",
       " 'Rural population (% of total population)',\n",
       " 'Rural population growth (annual %)',\n",
       " 'Rural poverty headcount ratio at national poverty lines (% of rural population)',\n",
       " 'School enrollment, primary (% gross)',\n",
       " 'School enrollment, primary (% net)',\n",
       " 'School enrollment, primary, female (% gross)',\n",
       " 'School enrollment, primary, female (% net)',\n",
       " 'School enrollment, primary, male (% gross)',\n",
       " 'School enrollment, primary, male (% net)',\n",
       " 'School enrollment, secondary (% gross)',\n",
       " 'School enrollment, secondary (% net)',\n",
       " 'School enrollment, secondary, female (% gross)',\n",
       " 'School enrollment, secondary, female (% net)',\n",
       " 'School enrollment, secondary, male (% gross)',\n",
       " 'School enrollment, secondary, male (% net)',\n",
       " 'School enrollment, tertiary (% gross)',\n",
       " 'School enrollment, tertiary, female (% gross)',\n",
       " 'Sex ratio at birth (male births per female births)',\n",
       " 'Share of women employed in the nonagricultural sector (% of total nonagricultural employment)',\n",
       " 'Specialist surgical workforce (per 100,000 population)',\n",
       " 'Stillbirth rate (per 1,000 total births)',\n",
       " 'Suicide mortality rate (per 100,000 population)',\n",
       " 'Suicide mortality rate, female (per 100,000 female population)',\n",
       " 'Suicide mortality rate, male (per 100,000 male population)',\n",
       " 'Survival to age 65, female (% of cohort)',\n",
       " 'Survival to age 65, male (% of cohort)',\n",
       " 'Teenage mothers (% of women ages 15-19 who have had children or are currently pregnant)',\n",
       " 'Total alcohol consumption per capita (liters of pure alcohol, projected estimates, 15+ years of age)',\n",
       " 'Total alcohol consumption per capita, female (liters of pure alcohol, projected estimates, female 15+ years of age)',\n",
       " 'Total alcohol consumption per capita, male (liters of pure alcohol, projected estimates, male 15+ years of age)',\n",
       " 'Treatment for hypertension (% of adults ages 30-79 with hypertension)',\n",
       " 'Treatment for hypertension, female (% of female adults ages 30-79 with hypertension)',\n",
       " 'Treatment for hypertension, male (% of male adults ages 30-79 with hypertension)',\n",
       " 'Tuberculosis case detection rate (%, all forms)',\n",
       " 'Tuberculosis death rate (per 100,000 people)',\n",
       " 'Tuberculosis treatment success rate (% of new cases)',\n",
       " 'UHC service coverage index',\n",
       " 'Unemployment, female (% of female labor force)',\n",
       " 'Unemployment, male (% of male labor force)',\n",
       " 'Unemployment, total (% of total labor force)',\n",
       " 'Unmet need for contraception (% of married women ages 15-49)',\n",
       " 'Urban population',\n",
       " 'Urban population (% of total population)',\n",
       " 'Urban population growth (annual %)',\n",
       " 'Urban poverty headcount ratio at national poverty lines (% of urban population)',\n",
       " 'Use of insecticide-treated bed nets (% of under-5 population)',\n",
       " 'Vitamin A supplementation coverage rate (% of children ages 6-59 months)',\n",
       " 'Wanted fertility rate (births per woman)',\n",
       " 'Women who were first married by age 15 (% of women ages 20-24)',\n",
       " 'Women who were first married by age 18 (% of women ages 20-24)',\n",
       " \"Women's share of population ages 15+ living with HIV (%)\",\n",
       " 'Young people (ages 15-24) newly infected with HIV']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filterdCountryDf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e399816e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = 'Primary completion rate, total (% of relevant age group)'\n",
    "first = filterdCountryDf.filter(fn.col(cl).isNotNull()).groupBy(\"Country Name\").agg(fn.count(\"*\").alias(\"nonnull\"))\n",
    "second = filterdCountryDf.filter(fn.col(cl).isNull()).groupBy(\"Country Name\").agg(fn.count(\"*\").alias(\"null\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3d80422a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----+-------+-----+\n",
      "|Country Name |null|nonnull|ratio|\n",
      "+-------------+----+-------+-----+\n",
      "|Canada       |null|16     |null |\n",
      "|Thailand     |null|16     |null |\n",
      "|Mexico       |null|16     |null |\n",
      "|Madagascar   |null|16     |null |\n",
      "|Guinea       |null|16     |null |\n",
      "|India        |null|16     |null |\n",
      "|China        |null|16     |null |\n",
      "|United States|null|16     |null |\n",
      "|Niger        |null|16     |null |\n",
      "+-------------+----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "second.join(first,[\"Country Name\"],'right').withColumn(\"ratio\",fn.col(\"nonnull\")/fn.col(\"null\")).orderBy(fn.desc(\"ratio\")).show(300,False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
